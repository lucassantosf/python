{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb6e90a-ae5d-4b14-99d0-3c668cd779da",
   "metadata": {},
   "source": [
    "# Previsão temporal - preços da bolsa de valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8eeeb-bbbb-4fb2-8d17-95cdb23165bd",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e184b0e-32a4-4805-a47f-4e7e499ca278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacao desta lib para desativar erro no TensorFlow\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5674851-e883-4e2b-8a46-191012c19f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 16:35:51.783788: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-15 16:35:51.787831: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-15 16:35:51.841805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 16:35:52.964029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5ef04c-bb08-4940-abf8-180a2b76d996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.2.2', '2.16.1', '3.8.4', '1.4.2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, pd.__version__, tf.__version__, matplotlib.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8814e86d-2c49-444a-ad93-ed0144a2f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5613a-a694-4474-a8b8-53dae35bfe6d",
   "metadata": {},
   "source": [
    "## Carregamento base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae312f5-ada8-4af3-80ac-2b4c83586e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.718563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>15.970000</td>\n",
       "      <td>15.938125</td>\n",
       "      <td>22173100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>15.990000</td>\n",
       "      <td>16.139999</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>16.049999</td>\n",
       "      <td>16.017963</td>\n",
       "      <td>23552200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>19011500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>16.067865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1245 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       "1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       "1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       "1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       "1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       "\n",
       "          Volume  \n",
       "0     30182600.0  \n",
       "1     30552600.0  \n",
       "2     36141000.0  \n",
       "3     28069600.0  \n",
       "4     29091300.0  \n",
       "...          ...  \n",
       "1240         0.0  \n",
       "1241  22173100.0  \n",
       "1242  23552200.0  \n",
       "1243  19011500.0  \n",
       "1244         0.0  \n",
       "\n",
       "[1245 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = pd.read_csv('petr4-treinamento.csv')\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15301b20-af73-404b-8707-c857c46b2495",
   "metadata": {},
   "source": [
    "## Verificar se há dados faltantes na base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97a554e-a1e9-45c4-b279-f86753f9237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         3\n",
       "High         3\n",
       "Low          3\n",
       "Close        3\n",
       "Adj Close    3\n",
       "Volume       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27fcaaa-881b-4ced-a111-66d50b7055cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = base.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b0dbf1-6b31-4f97-8263-a01560afc82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21367f52-0024-4a89-89e8-89bcbb8889ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7cb5730-f533-482c-9d61-f71849ee81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = base.iloc[:,1:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3728f64-fabd-4a2b-a179-ab9680c98282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9990000e+01, 2.0209999e+01, 1.9690001e+01, 1.9690001e+01,\n",
       "        1.8086271e+01, 3.0182600e+07],\n",
       "       [1.9809999e+01, 2.0400000e+01, 1.9700001e+01, 2.0400000e+01,\n",
       "        1.8738441e+01, 3.0552600e+07],\n",
       "       [2.0330000e+01, 2.0620001e+01, 2.0170000e+01, 2.0430000e+01,\n",
       "        1.8766001e+01, 3.6141000e+07],\n",
       "       ...,\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10896a-fdf6-47a6-86e3-d690ea29614b",
   "metadata": {},
   "source": [
    "## Normalização dos dados - escalonamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e237a4-5905-4d6f-bec6-4ed4c2a13e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "        0.04318274],\n",
       "       [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "        0.0437121 ],\n",
       "       [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "        0.05170752],\n",
       "       ...,\n",
       "       [0.57122093, 0.57537562, 0.60696008, 0.58202356, 0.58202349,\n",
       "        0.03369652],\n",
       "       [0.57655039, 0.57489089, 0.60798362, 0.5844794 , 0.58447937,\n",
       "        0.02720006],\n",
       "       [0.57655039, 0.57343674, 0.61310133, 0.5844794 , 0.58447937,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "base_treinamento_normalizada = normalizador.fit_transform(base_treinamento)\n",
    "base_treinamento_normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7db5642-2f44-4ac4-99b2-b710808fa570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76501938],\n",
       "       [0.7562984 ],\n",
       "       [0.78149225],\n",
       "       ...,\n",
       "       [0.57122093],\n",
       "       [0.57655039],\n",
       "       [0.57655039]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Este normalizador servirá apenas para a base de teste de previsores\n",
    "normalizador_previsao = MinMaxScaler(feature_range=(0,1))\n",
    "normalizador_previsao.fit_transform(base_treinamento[:,0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cee5a9-740a-413c-974d-11c6b39fcf9d",
   "metadata": {},
   "source": [
    "## Separacao base de previsores e alvo com base nos ultimos 90 registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64bbcb0-4259-453e-834c-227e474663cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1242, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_treinamento_normalizada.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b505e6cf-4e37-4410-8c78-5dbde3a612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # previsores\n",
    "y = [] # alvo - preco real\n",
    "\n",
    "for i in range(90,1242): # 90 precos anteriores\n",
    "    X.append(base_treinamento_normalizada[i-90:i,0:6])\n",
    "    y.append(base_treinamento_normalizada[i,0])\n",
    "    #print(i,i-90)\n",
    "\n",
    "X,y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "544c59d4-f5b1-4969-ad9a-4a03f3eb38ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.76501938, 0.77266112, 0.79682707, 0.76080559, 0.6838135 ,\n",
       "         0.04318274],\n",
       "        [0.7562984 , 0.78187106, 0.79733884, 0.79567784, 0.71590949,\n",
       "         0.0437121 ],\n",
       "        [0.78149225, 0.79253519, 0.82139202, 0.79715132, 0.71726583,\n",
       "         0.05170752],\n",
       "        [0.78875969, 0.7949588 , 0.81013311, 0.77996075, 0.70144373,\n",
       "         0.04015963],\n",
       "        [0.77083338, 0.77363063, 0.78505624, 0.75147351, 0.67522435,\n",
       "         0.0416214 ],\n",
       "        [0.74806197, 0.75618037, 0.78505624, 0.76031438, 0.68336137,\n",
       "         0.03485382],\n",
       "        [0.75436047, 0.76490543, 0.78915051, 0.76768177, 0.69014234,\n",
       "         0.02507502],\n",
       "        [0.75823643, 0.76442079, 0.79733884, 0.77013751, 0.6924025 ,\n",
       "         0.0260728 ],\n",
       "        [0.76598837, 0.77411537, 0.79682707, 0.76227897, 0.68516964,\n",
       "         0.0404927 ],\n",
       "        [0.76598837, 0.77411537, 0.79682707, 0.76719061, 0.68969016,\n",
       "         0.0423977 ],\n",
       "        [0.76017437, 0.75714973, 0.79222108, 0.76817293, 0.69059437,\n",
       "         0.02401858],\n",
       "        [0.75872098, 0.75908871, 0.79222108, 0.76178781, 0.68471746,\n",
       "         0.02821315],\n",
       "        [0.75581391, 0.75714973, 0.78915051, 0.75540279, 0.6788408 ,\n",
       "         0.02706042],\n",
       "        [0.74467054, 0.74309258, 0.77533265, 0.74607071, 0.67025175,\n",
       "         0.02587622],\n",
       "        [0.7374031 , 0.74357736, 0.77328557, 0.75540279, 0.6788408 ,\n",
       "         0.03367205],\n",
       "        [0.7374031 , 0.74454673, 0.77328557, 0.75392926, 0.67748471,\n",
       "         0.02460946],\n",
       "        [0.73498067, 0.75036355, 0.78045041, 0.75687631, 0.68019705,\n",
       "         0.02806007],\n",
       "        [0.75242248, 0.75327189, 0.77533265, 0.74508849, 0.66934774,\n",
       "         0.02878973],\n",
       "        [0.73401163, 0.73194382, 0.75332651, 0.73231836, 0.65759427,\n",
       "         0.03876941],\n",
       "        [0.71656977, 0.71352399, 0.71903787, 0.68762287, 0.6164569 ,\n",
       "         0.09583767],\n",
       "        [0.68120155, 0.68153175, 0.70522006, 0.68172891, 0.61103237,\n",
       "         0.04756616],\n",
       "        [0.67538755, 0.69704314, 0.71647907, 0.70039291, 0.62821037,\n",
       "         0.04129104],\n",
       "        [0.67635659, 0.68250121, 0.70470824, 0.67779964, 0.60741587,\n",
       "         0.04620398],\n",
       "        [0.63372098, 0.67959287, 0.67246673, 0.68172891, 0.61103237,\n",
       "         0.11064144],\n",
       "        [0.66521318, 0.66553563, 0.6862846 , 0.65815327, 0.58933361,\n",
       "         0.04418925],\n",
       "        [0.65649225, 0.66456617, 0.67553736, 0.65324168, 0.584813  ,\n",
       "         0.0530315 ],\n",
       "        [0.64680228, 0.65487159, 0.67860793, 0.6650295 , 0.5956623 ,\n",
       "         0.04444964],\n",
       "        [0.66618222, 0.66553563, 0.69651996, 0.66797641, 0.59837464,\n",
       "         0.03194532],\n",
       "        [0.65843028, 0.66068832, 0.6888434 , 0.66159139, 0.59249793,\n",
       "         0.0370597 ],\n",
       "        [0.64970935, 0.65535633, 0.6862846 , 0.6596267 , 0.59068976,\n",
       "         0.0357702 ],\n",
       "        [0.65116274, 0.66311202, 0.68577277, 0.67288805, 0.60289526,\n",
       "         0.02903152],\n",
       "        [0.66424419, 0.67426079, 0.70470824, 0.68271123, 0.61193639,\n",
       "         0.0412361 ],\n",
       "        [0.67344961, 0.67038294, 0.68730803, 0.65913564, 0.59023768,\n",
       "         0.03711206],\n",
       "        [0.64292631, 0.6446922 , 0.66939616, 0.64440082, 0.57667593,\n",
       "         0.04346845],\n",
       "        [0.64486434, 0.64178381, 0.65967247, 0.63605111, 0.56899095,\n",
       "         0.04421171],\n",
       "        [0.62257747, 0.62190984, 0.65148414, 0.62622798, 0.55994986,\n",
       "         0.04364257],\n",
       "        [0.60949617, 0.61027635, 0.63510752, 0.61591359, 0.55045665,\n",
       "         0.04779322],\n",
       "        [0.60998067, 0.61609307, 0.6407369 , 0.61935165, 0.55362107,\n",
       "         0.04092922],\n",
       "        [0.60852713, 0.60979157, 0.63613096, 0.60952857, 0.54457989,\n",
       "         0.03981569],\n",
       "        [0.59593023, 0.61803199, 0.62845445, 0.62377213, 0.55768961,\n",
       "         0.04509603],\n",
       "        [0.61143411, 0.62190984, 0.63254862, 0.60412577, 0.5396073 ,\n",
       "         0.05085238],\n",
       "        [0.60222863, 0.60542899, 0.6320368 , 0.60707267, 0.54231954,\n",
       "         0.04531064],\n",
       "        [0.64922481, 0.67862336, 0.6704196 , 0.68025539, 0.60967603,\n",
       "         0.10572707],\n",
       "        [0.68362398, 0.74212312, 0.72620261, 0.72445981, 0.65036132,\n",
       "         0.08930445],\n",
       "        [0.70687989, 0.72952012, 0.7185261 , 0.69597258, 0.62414194,\n",
       "         0.04376518],\n",
       "        [0.68265509, 0.71255448, 0.7062436 , 0.72347744, 0.64945705,\n",
       "         0.03589495],\n",
       "        [0.70978682, 0.72079491, 0.74257927, 0.71414542, 0.64086801,\n",
       "         0.03739277],\n",
       "        [0.70784879, 0.72370339, 0.74769703, 0.71463658, 0.64132019,\n",
       "         0.04530406],\n",
       "        [0.71608527, 0.73242845, 0.74104401, 0.74115922, 0.66573124,\n",
       "         0.03887614],\n",
       "        [0.73643411, 0.74066888, 0.76202661, 0.73133599, 0.65669001,\n",
       "         0.06269313],\n",
       "        [0.7122093 , 0.73097431, 0.75332651, 0.73673879, 0.6616627 ,\n",
       "         0.05787405],\n",
       "        [0.7122093 , 0.73097431, 0.75281474, 0.73182715, 0.65714209,\n",
       "         0.04839097],\n",
       "        [0.7194767 , 0.72176442, 0.74513818, 0.71954817, 0.6458407 ,\n",
       "         0.03954013],\n",
       "        [0.70348832, 0.70722254, 0.73541453, 0.70383112, 0.63137489,\n",
       "         0.03144514],\n",
       "        [0.69525189, 0.69995148, 0.73387917, 0.70874262, 0.63589531,\n",
       "         0.02308847],\n",
       "        [0.70397287, 0.70528357, 0.73183214, 0.70677803, 0.63408723,\n",
       "         0.03482392],\n",
       "        [0.70397287, 0.7081919 , 0.73490276, 0.70677803, 0.63408723,\n",
       "         0.02257928],\n",
       "        [0.69767442, 0.69510427, 0.72824974, 0.69842833, 0.6264022 ,\n",
       "         0.01903582],\n",
       "        [0.68168605, 0.68395536, 0.71136131, 0.67927317, 0.60877212,\n",
       "         0.02224034],\n",
       "        [0.68168605, 0.68395536, 0.69344933, 0.66306491, 0.59385423,\n",
       "         0.02942397],\n",
       "        [0.65310078, 0.66650509, 0.69396111, 0.67779964, 0.60741587,\n",
       "         0.02244093],\n",
       "        [0.66618222, 0.67571493, 0.6949847 , 0.66355598, 0.59430621,\n",
       "         0.02782257],\n",
       "        [0.64825581, 0.66117305, 0.68730803, 0.66797641, 0.59837464,\n",
       "         0.02440802],\n",
       "        [0.66182175, 0.66117305, 0.6765609 , 0.64685666, 0.57893629,\n",
       "         0.03144357],\n",
       "        [0.64341085, 0.6776539 , 0.68372569, 0.68516703, 0.61419665,\n",
       "         0.04400526],\n",
       "        [0.67877902, 0.69704314, 0.71903787, 0.69842833, 0.6264022 ,\n",
       "         0.04546845],\n",
       "        [0.69137592, 0.69122642, 0.7036848 , 0.67730848, 0.60696374,\n",
       "         0.03177292],\n",
       "        [0.66569772, 0.66941348, 0.6862846 , 0.67583495, 0.6056075 ,\n",
       "         0.03919891],\n",
       "        [0.65406982, 0.6572952 , 0.665302  , 0.63998039, 0.57260735,\n",
       "         0.05120333],\n",
       "        [0.64292631, 0.65341735, 0.68116684, 0.66306491, 0.59385423,\n",
       "         0.03397579],\n",
       "        [0.64147292, 0.64614639, 0.65813715, 0.63703343, 0.56989516,\n",
       "         0.05635362],\n",
       "        [0.63565891, 0.66262729, 0.665302  , 0.66895878, 0.5992789 ,\n",
       "         0.04077971],\n",
       "        [0.67587209, 0.68880271, 0.70777897, 0.6969548 , 0.625046  ,\n",
       "         0.0548714 ],\n",
       "        [0.68653106, 0.70382942, 0.71903787, 0.71660126, 0.64312846,\n",
       "         0.03461346],\n",
       "        [0.70300383, 0.73921474, 0.74411464, 0.73280952, 0.6580463 ,\n",
       "         0.04969664],\n",
       "        [0.71996119, 0.74600097, 0.76202661, 0.74852661, 0.67251211,\n",
       "         0.04766145],\n",
       "        [0.73982553, 0.74745521, 0.76867958, 0.73526526, 0.66030651,\n",
       "         0.05031056],\n",
       "        [0.76550388, 0.79059622, 0.80962134, 0.79666016, 0.71681365,\n",
       "         0.10120858],\n",
       "        [0.74854651, 0.76732913, 0.7840328 , 0.7804519 , 0.71911682,\n",
       "         0.06567045],\n",
       "        [0.75823643, 0.79301983, 0.80501535, 0.78831045, 0.72648688,\n",
       "         0.04828195],\n",
       "        [0.78924419, 0.79447407, 0.80706238, 0.77455795, 0.71358928,\n",
       "         0.06152981],\n",
       "        [0.76598837, 0.78041692, 0.80348004, 0.79223972, 0.73017189,\n",
       "         0.04455508],\n",
       "        [0.78488372, 0.79835191, 0.82702155, 0.80648339, 0.74353023,\n",
       "         0.03775975],\n",
       "        [0.80184109, 0.80222976, 0.82395082, 0.79027514, 0.72832946,\n",
       "         0.03492235],\n",
       "        [0.77761628, 0.78768783, 0.81729785, 0.7907662 , 0.72879006,\n",
       "         0.03271233],\n",
       "        [0.77325581, 0.78138628, 0.79785051, 0.77406679, 0.71312854,\n",
       "         0.0315204 ],\n",
       "        [0.7562984 , 0.75521086, 0.78096208, 0.75098236, 0.69147899,\n",
       "         0.03087142],\n",
       "        [0.74273261, 0.74697043, 0.77430911, 0.75392926, 0.69424286,\n",
       "         0.04384244],\n",
       "        [0.74127907, 0.74503146, 0.77840328, 0.75491163, 0.6951641 ,\n",
       "         0.03128876],\n",
       "        [0.74224806, 0.76635967, 0.78505624, 0.76375249, 0.7034554 ,\n",
       "         0.03586405]]),\n",
       " (1152, 90, 6))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b32d0-584b-4fb9-a765-e2e0ab0b95c2",
   "metadata": {},
   "source": [
    "## Estrutura da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009c8b00-17f3-4723-9f24-19af598d5b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m42,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,451</span> (443.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m113,451\u001b[0m (443.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences=True, input_shape=(X.shape[1],6)))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences=True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(Dense(units=1,activation='linear'))\n",
    "\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "951a8935-bfce-4b61-8259-b66c45964319",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61a2e216-44dc-4c8d-8d0d-10d5c3029aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss',min_delta=1e-10,patience=10,verbose=True)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.2,patience=5,verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='pesos.keras',monitor='loss',save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bcf9f73-650b-4c7f-a444-e2e340503f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.0803 - mean_absolute_error: 0.2061\n",
      "Epoch 1: loss improved from inf to 0.03351, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 108ms/step - loss: 0.0790 - mean_absolute_error: 0.2040 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0115 - mean_absolute_error: 0.0797\n",
      "Epoch 2: loss improved from 0.03351 to 0.01045, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0115 - mean_absolute_error: 0.0797 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0073 - mean_absolute_error: 0.0655\n",
      "Epoch 3: loss improved from 0.01045 to 0.00745, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - loss: 0.0073 - mean_absolute_error: 0.0655 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.0068 - mean_absolute_error: 0.0639\n",
      "Epoch 4: loss improved from 0.00745 to 0.00700, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.0068 - mean_absolute_error: 0.0639 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0075 - mean_absolute_error: 0.0651\n",
      "Epoch 5: loss did not improve from 0.00700\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - loss: 0.0075 - mean_absolute_error: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0066 - mean_absolute_error: 0.0622\n",
      "Epoch 6: loss did not improve from 0.00700\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - loss: 0.0066 - mean_absolute_error: 0.0623 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0066 - mean_absolute_error: 0.0627\n",
      "Epoch 7: loss improved from 0.00700 to 0.00645, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.0066 - mean_absolute_error: 0.0627 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0055 - mean_absolute_error: 0.0558\n",
      "Epoch 8: loss improved from 0.00645 to 0.00574, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - loss: 0.0055 - mean_absolute_error: 0.0559 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0058 - mean_absolute_error: 0.0596\n",
      "Epoch 9: loss did not improve from 0.00574\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0058 - mean_absolute_error: 0.0596 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0055 - mean_absolute_error: 0.0558\n",
      "Epoch 10: loss did not improve from 0.00574\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0055 - mean_absolute_error: 0.0558 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0060 - mean_absolute_error: 0.0594\n",
      "Epoch 11: loss did not improve from 0.00574\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - loss: 0.0060 - mean_absolute_error: 0.0594 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 0.0046 - mean_absolute_error: 0.0505\n",
      "Epoch 12: loss improved from 0.00574 to 0.00464, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 0.0046 - mean_absolute_error: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.0048 - mean_absolute_error: 0.0524\n",
      "Epoch 13: loss did not improve from 0.00464\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 0.0048 - mean_absolute_error: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0044 - mean_absolute_error: 0.0499\n",
      "Epoch 14: loss did not improve from 0.00464\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 108ms/step - loss: 0.0044 - mean_absolute_error: 0.0500 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0061 - mean_absolute_error: 0.0605\n",
      "Epoch 15: loss did not improve from 0.00464\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0061 - mean_absolute_error: 0.0604 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0047 - mean_absolute_error: 0.0525\n",
      "Epoch 16: loss improved from 0.00464 to 0.00425, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0047 - mean_absolute_error: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.0055 - mean_absolute_error: 0.0561\n",
      "Epoch 17: loss did not improve from 0.00425\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0055 - mean_absolute_error: 0.0561 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0048 - mean_absolute_error: 0.0544\n",
      "Epoch 18: loss did not improve from 0.00425\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - loss: 0.0048 - mean_absolute_error: 0.0544 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0042 - mean_absolute_error: 0.0495\n",
      "Epoch 19: loss improved from 0.00425 to 0.00419, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0042 - mean_absolute_error: 0.0495 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0046 - mean_absolute_error: 0.0513\n",
      "Epoch 20: loss did not improve from 0.00419\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 0.0046 - mean_absolute_error: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0039 - mean_absolute_error: 0.0480\n",
      "Epoch 21: loss improved from 0.00419 to 0.00368, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.0039 - mean_absolute_error: 0.0480 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0041 - mean_absolute_error: 0.0473\n",
      "Epoch 22: loss did not improve from 0.00368\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 0.0041 - mean_absolute_error: 0.0473 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0042 - mean_absolute_error: 0.0473\n",
      "Epoch 23: loss did not improve from 0.00368\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - loss: 0.0041 - mean_absolute_error: 0.0473 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0037 - mean_absolute_error: 0.0461\n",
      "Epoch 24: loss did not improve from 0.00368\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - loss: 0.0037 - mean_absolute_error: 0.0461 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.0035 - mean_absolute_error: 0.0450\n",
      "Epoch 25: loss improved from 0.00368 to 0.00353, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - loss: 0.0035 - mean_absolute_error: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0040 - mean_absolute_error: 0.0473\n",
      "Epoch 26: loss did not improve from 0.00353\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.0040 - mean_absolute_error: 0.0472 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.0033 - mean_absolute_error: 0.0431\n",
      "Epoch 27: loss improved from 0.00353 to 0.00348, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - loss: 0.0033 - mean_absolute_error: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0034 - mean_absolute_error: 0.0432\n",
      "Epoch 28: loss improved from 0.00348 to 0.00324, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0034 - mean_absolute_error: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0030 - mean_absolute_error: 0.0409\n",
      "Epoch 29: loss improved from 0.00324 to 0.00302, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.0030 - mean_absolute_error: 0.0409 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0031 - mean_absolute_error: 0.0427\n",
      "Epoch 30: loss did not improve from 0.00302\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.0031 - mean_absolute_error: 0.0427 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0037 - mean_absolute_error: 0.0459\n",
      "Epoch 31: loss did not improve from 0.00302\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0037 - mean_absolute_error: 0.0459 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0031 - mean_absolute_error: 0.0421\n",
      "Epoch 32: loss improved from 0.00302 to 0.00291, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.0031 - mean_absolute_error: 0.0421 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0027 - mean_absolute_error: 0.0392\n",
      "Epoch 33: loss improved from 0.00291 to 0.00279, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.0027 - mean_absolute_error: 0.0393 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0032 - mean_absolute_error: 0.0437\n",
      "Epoch 34: loss did not improve from 0.00279\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - loss: 0.0032 - mean_absolute_error: 0.0437 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0032 - mean_absolute_error: 0.0422\n",
      "Epoch 35: loss did not improve from 0.00279\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 120ms/step - loss: 0.0032 - mean_absolute_error: 0.0422 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.0029 - mean_absolute_error: 0.0418\n",
      "Epoch 36: loss did not improve from 0.00279\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - loss: 0.0029 - mean_absolute_error: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0027 - mean_absolute_error: 0.0412\n",
      "Epoch 37: loss improved from 0.00279 to 0.00259, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - loss: 0.0027 - mean_absolute_error: 0.0412 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0031 - mean_absolute_error: 0.0418\n",
      "Epoch 38: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - loss: 0.0031 - mean_absolute_error: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.0028 - mean_absolute_error: 0.0392\n",
      "Epoch 39: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - loss: 0.0028 - mean_absolute_error: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0029 - mean_absolute_error: 0.0414\n",
      "Epoch 40: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.0029 - mean_absolute_error: 0.0414 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0029 - mean_absolute_error: 0.0404\n",
      "Epoch 41: loss did not improve from 0.00259\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - loss: 0.0029 - mean_absolute_error: 0.0403 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 0.0026 - mean_absolute_error: 0.0393\n",
      "Epoch 42: loss improved from 0.00259 to 0.00241, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 216ms/step - loss: 0.0026 - mean_absolute_error: 0.0393 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0022 - mean_absolute_error: 0.0357\n",
      "Epoch 43: loss improved from 0.00241 to 0.00236, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 134ms/step - loss: 0.0022 - mean_absolute_error: 0.0357 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0025 - mean_absolute_error: 0.0375\n",
      "Epoch 44: loss did not improve from 0.00236\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 0.0025 - mean_absolute_error: 0.0375 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.0026 - mean_absolute_error: 0.0378\n",
      "Epoch 45: loss did not improve from 0.00236\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 0.0026 - mean_absolute_error: 0.0378 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0021 - mean_absolute_error: 0.0341\n",
      "Epoch 46: loss did not improve from 0.00236\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 0.0021 - mean_absolute_error: 0.0342 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0026 - mean_absolute_error: 0.0389\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 47: loss did not improve from 0.00236\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.0026 - mean_absolute_error: 0.0389 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0023 - mean_absolute_error: 0.0352\n",
      "Epoch 48: loss improved from 0.00236 to 0.00229, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - loss: 0.0023 - mean_absolute_error: 0.0352 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.0021 - mean_absolute_error: 0.0351\n",
      "Epoch 49: loss improved from 0.00229 to 0.00205, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 0.0021 - mean_absolute_error: 0.0350 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0024 - mean_absolute_error: 0.0363\n",
      "Epoch 50: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.0024 - mean_absolute_error: 0.0363 - learning_rate: 2.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0022 - mean_absolute_error: 0.0351\n",
      "Epoch 51: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 0.0022 - mean_absolute_error: 0.0351 - learning_rate: 2.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.0022 - mean_absolute_error: 0.0358\n",
      "Epoch 52: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - loss: 0.0022 - mean_absolute_error: 0.0358 - learning_rate: 2.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0017 - mean_absolute_error: 0.0312\n",
      "Epoch 53: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0017 - mean_absolute_error: 0.0312 - learning_rate: 2.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0019 - mean_absolute_error: 0.0331\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 54: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 0.0019 - mean_absolute_error: 0.0331 - learning_rate: 2.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0019 - mean_absolute_error: 0.0335\n",
      "Epoch 55: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 0.0019 - mean_absolute_error: 0.0335 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0020 - mean_absolute_error: 0.0336\n",
      "Epoch 56: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - loss: 0.0020 - mean_absolute_error: 0.0336 - learning_rate: 4.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0021 - mean_absolute_error: 0.0339\n",
      "Epoch 57: loss did not improve from 0.00205\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - loss: 0.0021 - mean_absolute_error: 0.0339 - learning_rate: 4.0000e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.0017 - mean_absolute_error: 0.0311\n",
      "Epoch 58: loss improved from 0.00205 to 0.00193, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - loss: 0.0017 - mean_absolute_error: 0.0311 - learning_rate: 4.0000e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.0020 - mean_absolute_error: 0.0342\n",
      "Epoch 59: loss did not improve from 0.00193\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.0020 - mean_absolute_error: 0.0342 - learning_rate: 4.0000e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.0018 - mean_absolute_error: 0.0317\n",
      "Epoch 60: loss improved from 0.00193 to 0.00190, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - loss: 0.0018 - mean_absolute_error: 0.0317 - learning_rate: 4.0000e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0022 - mean_absolute_error: 0.0350\n",
      "Epoch 61: loss did not improve from 0.00190\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.0022 - mean_absolute_error: 0.0350 - learning_rate: 4.0000e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0019 - mean_absolute_error: 0.0329\n",
      "Epoch 62: loss improved from 0.00190 to 0.00189, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - loss: 0.0019 - mean_absolute_error: 0.0329 - learning_rate: 4.0000e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.0019 - mean_absolute_error: 0.0335\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 63: loss did not improve from 0.00189\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 0.0019 - mean_absolute_error: 0.0335 - learning_rate: 4.0000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.0018 - mean_absolute_error: 0.0329\n",
      "Epoch 64: loss did not improve from 0.00189\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - loss: 0.0018 - mean_absolute_error: 0.0329 - learning_rate: 8.0000e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.0020 - mean_absolute_error: 0.0339\n",
      "Epoch 65: loss did not improve from 0.00189\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - loss: 0.0020 - mean_absolute_error: 0.0339 - learning_rate: 8.0000e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 0.0020 - mean_absolute_error: 0.0342\n",
      "Epoch 66: loss improved from 0.00189 to 0.00186, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 166ms/step - loss: 0.0020 - mean_absolute_error: 0.0341 - learning_rate: 8.0000e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.0023 - mean_absolute_error: 0.0350\n",
      "Epoch 67: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 0.0023 - mean_absolute_error: 0.0349 - learning_rate: 8.0000e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 0.0021 - mean_absolute_error: 0.0348\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 68: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 178ms/step - loss: 0.0021 - mean_absolute_error: 0.0347 - learning_rate: 8.0000e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0017 - mean_absolute_error: 0.0318\n",
      "Epoch 69: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - loss: 0.0017 - mean_absolute_error: 0.0318 - learning_rate: 1.6000e-06\n",
      "Epoch 70/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.0019 - mean_absolute_error: 0.0323\n",
      "Epoch 70: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - loss: 0.0019 - mean_absolute_error: 0.0323 - learning_rate: 1.6000e-06\n",
      "Epoch 71/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0021 - mean_absolute_error: 0.0345\n",
      "Epoch 71: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0021 - mean_absolute_error: 0.0345 - learning_rate: 1.6000e-06\n",
      "Epoch 72/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0018 - mean_absolute_error: 0.0322\n",
      "Epoch 72: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0018 - mean_absolute_error: 0.0322 - learning_rate: 1.6000e-06\n",
      "Epoch 73/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0019 - mean_absolute_error: 0.0325\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "\n",
      "Epoch 73: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0019 - mean_absolute_error: 0.0325 - learning_rate: 1.6000e-06\n",
      "Epoch 74/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0022 - mean_absolute_error: 0.0356\n",
      "Epoch 74: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0022 - mean_absolute_error: 0.0356 - learning_rate: 3.2000e-07\n",
      "Epoch 75/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0018 - mean_absolute_error: 0.0316\n",
      "Epoch 75: loss improved from 0.00186 to 0.00186, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.0018 - mean_absolute_error: 0.0317 - learning_rate: 3.2000e-07\n",
      "Epoch 76/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0021 - mean_absolute_error: 0.0347\n",
      "Epoch 76: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0021 - mean_absolute_error: 0.0346 - learning_rate: 3.2000e-07\n",
      "Epoch 77/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0018 - mean_absolute_error: 0.0326\n",
      "Epoch 77: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.0018 - mean_absolute_error: 0.0326 - learning_rate: 3.2000e-07\n",
      "Epoch 78/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0019 - mean_absolute_error: 0.0331\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "\n",
      "Epoch 78: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - loss: 0.0019 - mean_absolute_error: 0.0331 - learning_rate: 3.2000e-07\n",
      "Epoch 79/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0022 - mean_absolute_error: 0.0346\n",
      "Epoch 79: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0022 - mean_absolute_error: 0.0346 - learning_rate: 6.4000e-08\n",
      "Epoch 80/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0018 - mean_absolute_error: 0.0313\n",
      "Epoch 80: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 0.0018 - mean_absolute_error: 0.0313 - learning_rate: 6.4000e-08\n",
      "Epoch 81/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0019 - mean_absolute_error: 0.0324\n",
      "Epoch 81: loss did not improve from 0.00186\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0019 - mean_absolute_error: 0.0324 - learning_rate: 6.4000e-08\n",
      "Epoch 82/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0017 - mean_absolute_error: 0.0316\n",
      "Epoch 82: loss improved from 0.00186 to 0.00178, saving model to pesos.keras\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - loss: 0.0017 - mean_absolute_error: 0.0316 - learning_rate: 6.4000e-08\n",
      "Epoch 83/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0018 - mean_absolute_error: 0.0318\n",
      "Epoch 83: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.0018 - mean_absolute_error: 0.0318 - learning_rate: 6.4000e-08\n",
      "Epoch 84/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0020 - mean_absolute_error: 0.0333\n",
      "Epoch 84: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 0.0020 - mean_absolute_error: 0.0333 - learning_rate: 6.4000e-08\n",
      "Epoch 85/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0024 - mean_absolute_error: 0.0358\n",
      "Epoch 85: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0024 - mean_absolute_error: 0.0357 - learning_rate: 6.4000e-08\n",
      "Epoch 86/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0022 - mean_absolute_error: 0.0353\n",
      "Epoch 86: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0022 - mean_absolute_error: 0.0352 - learning_rate: 6.4000e-08\n",
      "Epoch 87/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0019 - mean_absolute_error: 0.0329\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "\n",
      "Epoch 87: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.0019 - mean_absolute_error: 0.0330 - learning_rate: 6.4000e-08\n",
      "Epoch 88/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0356\n",
      "Epoch 88: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0023 - mean_absolute_error: 0.0356 - learning_rate: 1.2800e-08\n",
      "Epoch 89/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0019 - mean_absolute_error: 0.0330\n",
      "Epoch 89: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0019 - mean_absolute_error: 0.0330 - learning_rate: 1.2800e-08\n",
      "Epoch 90/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0022 - mean_absolute_error: 0.0347\n",
      "Epoch 90: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step - loss: 0.0022 - mean_absolute_error: 0.0347 - learning_rate: 1.2800e-08\n",
      "Epoch 91/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0022 - mean_absolute_error: 0.0358\n",
      "Epoch 91: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0022 - mean_absolute_error: 0.0358 - learning_rate: 1.2800e-08\n",
      "Epoch 92/100\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0020 - mean_absolute_error: 0.0333\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "\n",
      "Epoch 92: loss did not improve from 0.00178\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0020 - mean_absolute_error: 0.0334 - learning_rate: 1.2800e-08\n",
      "Epoch 92: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x733c0d9a8410>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X,y,epochs=100,batch_size=32,callbacks=[es,rlr,mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49612d6f-5dfd-48a5-b4a3-17e1b4ac2354",
   "metadata": {},
   "source": [
    "## Previsoes com Base de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac280fde-b4ec-43fd-a270-0de812ef76b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.190001</td>\n",
       "      <td>16.549999</td>\n",
       "      <td>16.516966</td>\n",
       "      <td>33461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>16.490000</td>\n",
       "      <td>16.719999</td>\n",
       "      <td>16.370001</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.666668</td>\n",
       "      <td>55940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16.780001</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>16.620001</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>16.696608</td>\n",
       "      <td>37064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16.700001</td>\n",
       "      <td>16.860001</td>\n",
       "      <td>16.570000</td>\n",
       "      <td>16.830000</td>\n",
       "      <td>16.796408</td>\n",
       "      <td>26958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>16.740000</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>28400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>17.160000</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>16.996010</td>\n",
       "      <td>35070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>17.049999</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>16.799999</td>\n",
       "      <td>16.766466</td>\n",
       "      <td>28547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>16.879999</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>17.215569</td>\n",
       "      <td>37921500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>17.040001</td>\n",
       "      <td>17.410000</td>\n",
       "      <td>17.020000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.265469</td>\n",
       "      <td>45912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.150000</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.315371</td>\n",
       "      <td>28945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>17.840000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>17.614771</td>\n",
       "      <td>58618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>17.920000</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>17.809999</td>\n",
       "      <td>18.360001</td>\n",
       "      <td>18.323355</td>\n",
       "      <td>58488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>18.530001</td>\n",
       "      <td>17.930000</td>\n",
       "      <td>18.219999</td>\n",
       "      <td>18.183632</td>\n",
       "      <td>48575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>18.030001</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.223553</td>\n",
       "      <td>33470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>18.260000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.090000</td>\n",
       "      <td>18.469999</td>\n",
       "      <td>18.433134</td>\n",
       "      <td>33920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-01-23</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.240000</td>\n",
       "      <td>18.203592</td>\n",
       "      <td>35567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>89768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       "1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       "2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       "3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       "4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       "5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       "6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       "7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       "8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       "9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       "10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       "11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       "12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       "13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       "14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       "15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       "16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "      Volume  \n",
       "0   33461800  \n",
       "1   55940900  \n",
       "2   37064900  \n",
       "3   26958200  \n",
       "4   28400000  \n",
       "5   35070900  \n",
       "6   28547700  \n",
       "7   37921500  \n",
       "8   45912100  \n",
       "9   28945400  \n",
       "10  58618300  \n",
       "11  58488900  \n",
       "12  48575800  \n",
       "13  33470200  \n",
       "14  33920000  \n",
       "15  35567700  \n",
       "16  89768200  \n",
       "17         0  \n",
       "18  81989500  \n",
       "19  55726200  \n",
       "20  46203000  \n",
       "21  41576600  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_teste = pd.read_csv('petr4-teste.csv')\n",
    "base_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f36855-a8f0-4ed0-88c5-af9bcaaaf644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eda77bd3-e810-4103-83c5-5a0bbd9df998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste = base_teste.iloc[:,1:2].values\n",
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f332133-34b0-4773-a414-5a0f1290012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [base,base_teste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc4ca502-79e7-4279-9d2b-f8d36078c456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0     2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       " 1     2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       " 2     2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       " 3     2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       " 4     2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       " ...          ...        ...        ...        ...        ...        ...   \n",
       " 1240  2017-12-25  15.750000  15.750000  15.750000  15.750000  15.718563   \n",
       " 1241  2017-12-26  15.750000  15.990000  15.690000  15.970000  15.938125   \n",
       " 1242  2017-12-27  15.990000  16.139999  15.980000  16.049999  16.017963   \n",
       " 1243  2017-12-28  16.100000  16.129999  16.000000  16.100000  16.067865   \n",
       " 1244  2017-12-29  16.100000  16.100000  16.100000  16.100000  16.067865   \n",
       " \n",
       "           Volume  \n",
       " 0     30182600.0  \n",
       " 1     30552600.0  \n",
       " 2     36141000.0  \n",
       " 3     28069600.0  \n",
       " 4     29091300.0  \n",
       " ...          ...  \n",
       " 1240         0.0  \n",
       " 1241  22173100.0  \n",
       " 1242  23552200.0  \n",
       " 1243  19011500.0  \n",
       " 1244         0.0  \n",
       " \n",
       " [1242 rows x 7 columns],\n",
       "           Date       Open       High        Low      Close  Adj Close  \\\n",
       " 0   2018-01-02  16.190001  16.549999  16.190001  16.549999  16.516966   \n",
       " 1   2018-01-03  16.490000  16.719999  16.370001  16.700001  16.666668   \n",
       " 2   2018-01-04  16.780001  16.959999  16.620001  16.730000  16.696608   \n",
       " 3   2018-01-05  16.700001  16.860001  16.570000  16.830000  16.796408   \n",
       " 4   2018-01-08  16.740000  17.030001  16.709999  17.030001  16.996010   \n",
       " 5   2018-01-09  17.030001  17.160000  16.959999  17.030001  16.996010   \n",
       " 6   2018-01-10  16.920000  17.049999  16.770000  16.799999  16.766466   \n",
       " 7   2018-01-11  16.879999  17.299999  16.840000  17.250000  17.215569   \n",
       " 8   2018-01-12  17.040001  17.410000  17.020000  17.299999  17.265469   \n",
       " 9   2018-01-15  17.320000  17.440001  17.150000  17.350000  17.315371   \n",
       " 10  2018-01-16  17.350000  17.840000  17.299999  17.650000  17.614771   \n",
       " 11  2018-01-17  17.920000  18.360001  17.809999  18.360001  18.323355   \n",
       " 12  2018-01-18  18.350000  18.530001  17.930000  18.219999  18.183632   \n",
       " 13  2018-01-19  18.309999  18.420000  18.030001  18.260000  18.223553   \n",
       " 14  2018-01-22  18.260000  18.469999  18.090000  18.469999  18.433134   \n",
       " 15  2018-01-23  18.400000  18.459999  18.000000  18.240000  18.203592   \n",
       " 16  2018-01-24  18.420000  19.629999  18.420000  19.340000  19.301397   \n",
       " 17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       " 18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       " 19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       " 20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       " 21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       " \n",
       "       Volume  \n",
       " 0   33461800  \n",
       " 1   55940900  \n",
       " 2   37064900  \n",
       " 3   26958200  \n",
       " 4   28400000  \n",
       " 5   35070900  \n",
       " 6   28547700  \n",
       " 7   37921500  \n",
       " 8   45912100  \n",
       " 9   28945400  \n",
       " 10  58618300  \n",
       " 11  58488900  \n",
       " 12  48575800  \n",
       " 13  33470200  \n",
       " 14  33920000  \n",
       " 15  35567700  \n",
       " 16  89768200  \n",
       " 17         0  \n",
       " 18  81989500  \n",
       " 19  55726200  \n",
       " 20  46203000  \n",
       " 21  41576600  ]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f429bdf-609f-492e-9007-0fd75afabe0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>19.690001</td>\n",
       "      <td>18.086271</td>\n",
       "      <td>30182600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>19.809999</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>18.738441</td>\n",
       "      <td>30552600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>20.330000</td>\n",
       "      <td>20.620001</td>\n",
       "      <td>20.170000</td>\n",
       "      <td>20.430000</td>\n",
       "      <td>18.766001</td>\n",
       "      <td>36141000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>20.670000</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>18.444506</td>\n",
       "      <td>28069600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>20.110001</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>19.459999</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>17.911745</td>\n",
       "      <td>29091300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.301397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>19.620001</td>\n",
       "      <td>19.980000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.890221</td>\n",
       "      <td>81989500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>20.049999</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>19.810381</td>\n",
       "      <td>55726200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>19.360001</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>19.451097</td>\n",
       "      <td>46203000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>19.740000</td>\n",
       "      <td>19.930000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>19.660681</td>\n",
       "      <td>41576600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2013-01-02  19.990000  20.209999  19.690001  19.690001  18.086271   \n",
       "1   2013-01-03  19.809999  20.400000  19.700001  20.400000  18.738441   \n",
       "2   2013-01-04  20.330000  20.620001  20.170000  20.430000  18.766001   \n",
       "3   2013-01-07  20.480000  20.670000  19.950001  20.080000  18.444506   \n",
       "4   2013-01-08  20.110001  20.230000  19.459999  19.500000  17.911745   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "17  2018-01-25  19.340000  19.340000  19.340000  19.340000  19.301397   \n",
       "18  2018-01-26  19.620001  19.980000  19.100000  19.930000  19.890221   \n",
       "19  2018-01-29  19.670000  20.049999  19.570000  19.850000  19.810381   \n",
       "20  2018-01-30  19.770000  19.770000  19.360001  19.490000  19.451097   \n",
       "21  2018-01-31  19.740000  19.930000  19.680000  19.700001  19.660681   \n",
       "\n",
       "        Volume  \n",
       "0   30182600.0  \n",
       "1   30552600.0  \n",
       "2   36141000.0  \n",
       "3   28069600.0  \n",
       "4   29091300.0  \n",
       "..         ...  \n",
       "17         0.0  \n",
       "18  81989500.0  \n",
       "19  55726200.0  \n",
       "20  46203000.0  \n",
       "21  41576600.0  \n",
       "\n",
       "[1264 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_completa = pd.concat(frames)\n",
    "base_completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b18ab858-2879-4eb5-9b9e-18aa258f29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = base_completa.drop('Date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19511e9a-f3ba-4c04-a1c3-ccfd7642c98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 22)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_completa), len(base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a645fa1b-ede3-4edc-9d48-18bdd136924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = base_completa[len(base_completa) - len(base_teste) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a56eec26-bfe2-4ba6-930d-fe981ae68a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3930000e+01, 1.4030000e+01, 1.3760000e+01, 1.3870000e+01,\n",
       "        1.3842316e+01, 2.7208100e+07],\n",
       "       [1.3760000e+01, 1.3850000e+01, 1.3680000e+01, 1.3850000e+01,\n",
       "        1.3822356e+01, 2.7306400e+07],\n",
       "       [1.3790000e+01, 1.3900000e+01, 1.3440000e+01, 1.3450000e+01,\n",
       "        1.3423154e+01, 5.8871700e+07],\n",
       "       [1.3530000e+01, 1.3770000e+01, 1.3470000e+01, 1.3650000e+01,\n",
       "        1.3622754e+01, 8.2909400e+07],\n",
       "       [1.3850000e+01, 1.4190000e+01, 1.3820000e+01, 1.4020000e+01,\n",
       "        1.3992017e+01, 6.0260300e+07],\n",
       "       [1.3960000e+01, 1.4180000e+01, 1.3940000e+01, 1.4170000e+01,\n",
       "        1.4141717e+01, 1.8139300e+07],\n",
       "       [1.4570000e+01, 1.4650000e+01, 1.4230000e+01, 1.4410000e+01,\n",
       "        1.4381238e+01, 5.6476800e+07],\n",
       "       [1.4650000e+01, 1.5020000e+01, 1.4510000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 6.8418200e+07],\n",
       "       [1.5020000e+01, 1.5020000e+01, 1.5020000e+01, 1.5020000e+01,\n",
       "        1.4990021e+01, 0.0000000e+00],\n",
       "       [1.5100000e+01, 1.5150000e+01, 1.4690000e+01, 1.4710000e+01,\n",
       "        1.4680639e+01, 3.6337400e+07],\n",
       "       [1.4880000e+01, 1.5050000e+01, 1.4810000e+01, 1.4990000e+01,\n",
       "        1.4960080e+01, 3.4915900e+07],\n",
       "       [1.4980000e+01, 1.5160000e+01, 1.4860000e+01, 1.4870000e+01,\n",
       "        1.4840320e+01, 4.9702800e+07],\n",
       "       [1.4940000e+01, 1.5100000e+01, 1.4810000e+01, 1.5030000e+01,\n",
       "        1.5000000e+01, 3.7010200e+07],\n",
       "       [1.5030000e+01, 1.5260000e+01, 1.5020000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 3.4413800e+07],\n",
       "       [1.5070000e+01, 1.5170000e+01, 1.4990000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7784700e+07],\n",
       "       [1.5020000e+01, 1.5190000e+01, 1.4980000e+01, 1.5040000e+01,\n",
       "        1.5009980e+01, 4.7601200e+07],\n",
       "       [1.5100000e+01, 1.5170000e+01, 1.4920000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 3.5822100e+07],\n",
       "       [1.5250000e+01, 1.5880000e+01, 1.5070000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 8.0267000e+07],\n",
       "       [1.5850000e+01, 1.5960000e+01, 1.5580000e+01, 1.5670000e+01,\n",
       "        1.5638723e+01, 4.6258800e+07],\n",
       "       [1.5600000e+01, 1.5800000e+01, 1.5430000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.0928300e+07],\n",
       "       [1.5790000e+01, 1.5960000e+01, 1.5700000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.6733200e+07],\n",
       "       [1.5860000e+01, 1.5900000e+01, 1.5560000e+01, 1.5560000e+01,\n",
       "        1.5528943e+01, 3.7874200e+07],\n",
       "       [1.5700000e+01, 1.5720000e+01, 1.5110000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 4.1819300e+07],\n",
       "       [1.5370000e+01, 1.5500000e+01, 1.5220000e+01, 1.5340000e+01,\n",
       "        1.5309381e+01, 3.3829000e+07],\n",
       "       [1.5500000e+01, 1.5520000e+01, 1.5300000e+01, 1.5300000e+01,\n",
       "        1.5269462e+01, 2.8638300e+07],\n",
       "       [1.5190000e+01, 1.5400000e+01, 1.5060000e+01, 1.5400000e+01,\n",
       "        1.5369262e+01, 2.9826200e+07],\n",
       "       [1.5600000e+01, 1.5980000e+01, 1.5520000e+01, 1.5980000e+01,\n",
       "        1.5948104e+01, 5.0636700e+07],\n",
       "       [1.5900000e+01, 1.5940000e+01, 1.5650000e+01, 1.5660000e+01,\n",
       "        1.5628743e+01, 4.7798600e+07],\n",
       "       [1.5880000e+01, 1.6110001e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 5.5361300e+07],\n",
       "       [1.5660000e+01, 1.5770000e+01, 1.5540000e+01, 1.5690000e+01,\n",
       "        1.5658683e+01, 4.1741300e+07],\n",
       "       [1.5610000e+01, 1.5890000e+01, 1.5590000e+01, 1.5890000e+01,\n",
       "        1.5858284e+01, 2.7904700e+07],\n",
       "       [1.6129999e+01, 1.6190001e+01, 1.6010000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 4.7066600e+07],\n",
       "       [1.6170000e+01, 1.6250000e+01, 1.6010000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 4.0422100e+07],\n",
       "       [1.6080000e+01, 1.6080000e+01, 1.6080000e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 0.0000000e+00],\n",
       "       [1.6230000e+01, 1.6290001e+01, 1.6059999e+01, 1.6080000e+01,\n",
       "        1.6047905e+01, 2.4210000e+07],\n",
       "       [1.6160000e+01, 1.6260000e+01, 1.6000000e+01, 1.6120001e+01,\n",
       "        1.6087826e+01, 4.4699700e+07],\n",
       "       [1.6139999e+01, 1.6219999e+01, 1.6070000e+01, 1.6129999e+01,\n",
       "        1.6097803e+01, 2.5524800e+07],\n",
       "       [1.6219999e+01, 1.6280001e+01, 1.6129999e+01, 1.6160000e+01,\n",
       "        1.6127745e+01, 2.5706200e+07],\n",
       "       [1.6000000e+01, 1.6160000e+01, 1.5900000e+01, 1.6150000e+01,\n",
       "        1.6117765e+01, 2.4672800e+07],\n",
       "       [1.6190001e+01, 1.6389999e+01, 1.6170000e+01, 1.6219999e+01,\n",
       "        1.6187624e+01, 3.2417500e+07],\n",
       "       [1.6290001e+01, 1.6290001e+01, 1.6120001e+01, 1.6200001e+01,\n",
       "        1.6167665e+01, 2.9389900e+07],\n",
       "       [1.6290001e+01, 1.6510000e+01, 1.6120001e+01, 1.6510000e+01,\n",
       "        1.6477047e+01, 4.6249500e+07],\n",
       "       [1.6530001e+01, 1.6730000e+01, 1.6450001e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.7608200e+07],\n",
       "       [1.6780001e+01, 1.6889999e+01, 1.6660000e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7848300e+07],\n",
       "       [1.6770000e+01, 1.7090000e+01, 1.6650000e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 4.5640100e+07],\n",
       "       [1.6969999e+01, 1.7170000e+01, 1.6740000e+01, 1.6780001e+01,\n",
       "        1.6746508e+01, 5.5355600e+07],\n",
       "       [1.6900000e+01, 1.6950001e+01, 1.6719999e+01, 1.6770000e+01,\n",
       "        1.6736528e+01, 3.2249000e+07],\n",
       "       [1.6990000e+01, 1.7100000e+01, 1.6879999e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 3.8876600e+07],\n",
       "       [1.6900000e+01, 1.6900000e+01, 1.6900000e+01, 1.6900000e+01,\n",
       "        1.6866268e+01, 0.0000000e+00],\n",
       "       [1.6959999e+01, 1.7010000e+01, 1.6680000e+01, 1.6940001e+01,\n",
       "        1.6906189e+01, 3.2605400e+07],\n",
       "       [1.7049999e+01, 1.7440001e+01, 1.6980000e+01, 1.7430000e+01,\n",
       "        1.7395210e+01, 4.6056100e+07],\n",
       "       [1.7309999e+01, 1.7350000e+01, 1.6500000e+01, 1.6500000e+01,\n",
       "        1.6467066e+01, 6.1098400e+07],\n",
       "       [1.6690001e+01, 1.6950001e+01, 1.6510000e+01, 1.6950001e+01,\n",
       "        1.6916168e+01, 4.1179600e+07],\n",
       "       [1.6889999e+01, 1.6940001e+01, 1.6719999e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 2.9399400e+07],\n",
       "       [1.6709999e+01, 1.6809999e+01, 1.6510000e+01, 1.6719999e+01,\n",
       "        1.6686626e+01, 3.5959400e+07],\n",
       "       [1.6690001e+01, 1.6770000e+01, 1.6389999e+01, 1.6639999e+01,\n",
       "        1.6606787e+01, 2.8697700e+07],\n",
       "       [1.6639999e+01, 1.6639999e+01, 1.5280000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 8.8765600e+07],\n",
       "       [1.5350000e+01, 1.5350000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 0.0000000e+00],\n",
       "       [1.5620000e+01, 1.6040001e+01, 1.5480000e+01, 1.5810000e+01,\n",
       "        1.5778444e+01, 4.2703800e+07],\n",
       "       [1.5920000e+01, 1.6120001e+01, 1.5810000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 3.8376900e+07],\n",
       "       [1.6020000e+01, 1.6020000e+01, 1.6020000e+01, 1.6020000e+01,\n",
       "        1.5988025e+01, 0.0000000e+00],\n",
       "       [1.6150000e+01, 1.6309999e+01, 1.5850000e+01, 1.5900000e+01,\n",
       "        1.5868263e+01, 4.5817800e+07],\n",
       "       [1.6090000e+01, 1.6240000e+01, 1.5930000e+01, 1.6110001e+01,\n",
       "        1.6077845e+01, 3.7444900e+07],\n",
       "       [1.5980000e+01, 1.6260000e+01, 1.5940000e+01, 1.6190001e+01,\n",
       "        1.6157686e+01, 1.5403600e+07],\n",
       "       [1.6250000e+01, 1.6370001e+01, 1.6040001e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.8790700e+07],\n",
       "       [1.6010000e+01, 1.6020000e+01, 1.5780000e+01, 1.5870000e+01,\n",
       "        1.5838324e+01, 2.8445800e+07],\n",
       "       [1.5930000e+01, 1.6040001e+01, 1.5810000e+01, 1.5840000e+01,\n",
       "        1.5808384e+01, 3.0429600e+07],\n",
       "       [1.5870000e+01, 1.5920000e+01, 1.5320000e+01, 1.5330000e+01,\n",
       "        1.5299401e+01, 4.5973000e+07],\n",
       "       [1.5300000e+01, 1.5470000e+01, 1.4990000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 5.2811400e+07],\n",
       "       [1.5340000e+01, 1.5770000e+01, 1.5260000e+01, 1.5610000e+01,\n",
       "        1.5578842e+01, 4.2703800e+07],\n",
       "       [1.5650000e+01, 1.5800000e+01, 1.5460000e+01, 1.5480000e+01,\n",
       "        1.5449101e+01, 4.3821500e+07],\n",
       "       [1.5500000e+01, 1.5830000e+01, 1.5210000e+01, 1.5310000e+01,\n",
       "        1.5279442e+01, 3.0228000e+07],\n",
       "       [1.5220000e+01, 1.5700000e+01, 1.5140000e+01, 1.5520000e+01,\n",
       "        1.5489023e+01, 3.9238500e+07],\n",
       "       [1.5300000e+01, 1.5490000e+01, 1.5070000e+01, 1.5260000e+01,\n",
       "        1.5229542e+01, 3.7281400e+07],\n",
       "       [1.5510000e+01, 1.5680000e+01, 1.5350000e+01, 1.5350000e+01,\n",
       "        1.5319362e+01, 3.9584500e+07],\n",
       "       [1.5480000e+01, 1.5570000e+01, 1.5370000e+01, 1.5380000e+01,\n",
       "        1.5349302e+01, 2.1281600e+07],\n",
       "       [1.5360000e+01, 1.5490000e+01, 1.5180000e+01, 1.5490000e+01,\n",
       "        1.5459082e+01, 3.6201200e+07],\n",
       "       [1.5650000e+01, 1.5680000e+01, 1.5110000e+01, 1.5180000e+01,\n",
       "        1.5149701e+01, 4.6828900e+07],\n",
       "       [1.5100000e+01, 1.5310000e+01, 1.5000000e+01, 1.5010000e+01,\n",
       "        1.4980041e+01, 3.7177300e+07],\n",
       "       [1.5050000e+01, 1.5240000e+01, 1.4950000e+01, 1.4950000e+01,\n",
       "        1.4920160e+01, 5.5668300e+07],\n",
       "       [1.5160000e+01, 1.5330000e+01, 1.5130000e+01, 1.5220000e+01,\n",
       "        1.5189621e+01, 4.2760400e+07],\n",
       "       [1.5180000e+01, 1.5250000e+01, 1.5060000e+01, 1.5140000e+01,\n",
       "        1.5109781e+01, 2.2639700e+07],\n",
       "       [1.5210000e+01, 1.5300000e+01, 1.5170000e+01, 1.5240000e+01,\n",
       "        1.5209581e+01, 2.0149700e+07],\n",
       "       [1.5310000e+01, 1.5870000e+01, 1.5300000e+01, 1.5860000e+01,\n",
       "        1.5828343e+01, 4.7219400e+07],\n",
       "       [1.5750000e+01, 1.5890000e+01, 1.5690000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 1.8708500e+07],\n",
       "       [1.5750000e+01, 1.5750000e+01, 1.5750000e+01, 1.5750000e+01,\n",
       "        1.5718563e+01, 0.0000000e+00],\n",
       "       [1.5750000e+01, 1.5990000e+01, 1.5690000e+01, 1.5970000e+01,\n",
       "        1.5938125e+01, 2.2173100e+07],\n",
       "       [1.5990000e+01, 1.6139999e+01, 1.5980000e+01, 1.6049999e+01,\n",
       "        1.6017963e+01, 2.3552200e+07],\n",
       "       [1.6100000e+01, 1.6129999e+01, 1.6000000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 1.9011500e+07],\n",
       "       [1.6100000e+01, 1.6100000e+01, 1.6100000e+01, 1.6100000e+01,\n",
       "        1.6067865e+01, 0.0000000e+00],\n",
       "       [1.6190001e+01, 1.6549999e+01, 1.6190001e+01, 1.6549999e+01,\n",
       "        1.6516966e+01, 3.3461800e+07],\n",
       "       [1.6490000e+01, 1.6719999e+01, 1.6370001e+01, 1.6700001e+01,\n",
       "        1.6666668e+01, 5.5940900e+07],\n",
       "       [1.6780001e+01, 1.6959999e+01, 1.6620001e+01, 1.6730000e+01,\n",
       "        1.6696608e+01, 3.7064900e+07],\n",
       "       [1.6700001e+01, 1.6860001e+01, 1.6570000e+01, 1.6830000e+01,\n",
       "        1.6796408e+01, 2.6958200e+07],\n",
       "       [1.6740000e+01, 1.7030001e+01, 1.6709999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 2.8400000e+07],\n",
       "       [1.7030001e+01, 1.7160000e+01, 1.6959999e+01, 1.7030001e+01,\n",
       "        1.6996010e+01, 3.5070900e+07],\n",
       "       [1.6920000e+01, 1.7049999e+01, 1.6770000e+01, 1.6799999e+01,\n",
       "        1.6766466e+01, 2.8547700e+07],\n",
       "       [1.6879999e+01, 1.7299999e+01, 1.6840000e+01, 1.7250000e+01,\n",
       "        1.7215569e+01, 3.7921500e+07],\n",
       "       [1.7040001e+01, 1.7410000e+01, 1.7020000e+01, 1.7299999e+01,\n",
       "        1.7265469e+01, 4.5912100e+07],\n",
       "       [1.7320000e+01, 1.7440001e+01, 1.7150000e+01, 1.7350000e+01,\n",
       "        1.7315371e+01, 2.8945400e+07],\n",
       "       [1.7350000e+01, 1.7840000e+01, 1.7299999e+01, 1.7650000e+01,\n",
       "        1.7614771e+01, 5.8618300e+07],\n",
       "       [1.7920000e+01, 1.8360001e+01, 1.7809999e+01, 1.8360001e+01,\n",
       "        1.8323355e+01, 5.8488900e+07],\n",
       "       [1.8350000e+01, 1.8530001e+01, 1.7930000e+01, 1.8219999e+01,\n",
       "        1.8183632e+01, 4.8575800e+07],\n",
       "       [1.8309999e+01, 1.8420000e+01, 1.8030001e+01, 1.8260000e+01,\n",
       "        1.8223553e+01, 3.3470200e+07],\n",
       "       [1.8260000e+01, 1.8469999e+01, 1.8090000e+01, 1.8469999e+01,\n",
       "        1.8433134e+01, 3.3920000e+07],\n",
       "       [1.8400000e+01, 1.8459999e+01, 1.8000000e+01, 1.8240000e+01,\n",
       "        1.8203592e+01, 3.5567700e+07],\n",
       "       [1.8420000e+01, 1.9629999e+01, 1.8420000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 8.9768200e+07],\n",
       "       [1.9340000e+01, 1.9340000e+01, 1.9340000e+01, 1.9340000e+01,\n",
       "        1.9301397e+01, 0.0000000e+00],\n",
       "       [1.9620001e+01, 1.9980000e+01, 1.9100000e+01, 1.9930000e+01,\n",
       "        1.9890221e+01, 8.1989500e+07],\n",
       "       [1.9670000e+01, 2.0049999e+01, 1.9570000e+01, 1.9850000e+01,\n",
       "        1.9810381e+01, 5.5726200e+07],\n",
       "       [1.9770000e+01, 1.9770000e+01, 1.9360001e+01, 1.9490000e+01,\n",
       "        1.9451097e+01, 4.6203000e+07],\n",
       "       [1.9740000e+01, 1.9930000e+01, 1.9680000e+01, 1.9700001e+01,\n",
       "        1.9660681e+01, 4.1576600e+07]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9202499-328a-4604-926c-575464eaebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = normalizador.transform(entradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5690904-5b0a-4f29-bfd5-72115381adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste = []\n",
    "for i in range(90,112):\n",
    "    X_teste.append(entradas[i-90:i,0:6])\n",
    "X_teste = np.array(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d91cdad1-6f6d-4f75-a018-c4e14076d418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 90, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53311a39-ee7c-413c-8faf-7bf69eb29972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 572ms/step\n"
     ]
    }
   ],
   "source": [
    "previsoes = regressor.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da5ae82a-cda0-475a-bcd6-b7005a326a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5759106 ],\n",
       "       [0.57974267],\n",
       "       [0.58554435],\n",
       "       [0.5938424 ],\n",
       "       [0.60290945],\n",
       "       [0.6111263 ],\n",
       "       [0.61759585],\n",
       "       [0.6207661 ],\n",
       "       [0.62192047],\n",
       "       [0.62313056],\n",
       "       [0.6256299 ],\n",
       "       [0.63077956],\n",
       "       [0.64120674],\n",
       "       [0.6555438 ],\n",
       "       [0.66942483],\n",
       "       [0.6793901 ],\n",
       "       [0.6832954 ],\n",
       "       [0.68709105],\n",
       "       [0.6947372 ],\n",
       "       [0.7084788 ],\n",
       "       [0.7258557 ],\n",
       "       [0.7395624 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e66d5b28-c83e-478c-a9a4-57faee0d90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = normalizador_previsao.inverse_transform(previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f241a84-95ca-4c29-bd77-e39b4dbbd1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.086796],\n",
       "       [16.165888],\n",
       "       [16.285635],\n",
       "       [16.456907],\n",
       "       [16.64405 ],\n",
       "       [16.813646],\n",
       "       [16.947178],\n",
       "       [17.012611],\n",
       "       [17.036438],\n",
       "       [17.061415],\n",
       "       [17.113   ],\n",
       "       [17.21929 ],\n",
       "       [17.434507],\n",
       "       [17.730423],\n",
       "       [18.016928],\n",
       "       [18.22261 ],\n",
       "       [18.303217],\n",
       "       [18.38156 ],\n",
       "       [18.539375],\n",
       "       [18.823002],\n",
       "       [19.181662],\n",
       "       [19.464567]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a25b7b1-0f8f-428a-b8e8-ba3567b7832b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.190001],\n",
       "       [16.49    ],\n",
       "       [16.780001],\n",
       "       [16.700001],\n",
       "       [16.74    ],\n",
       "       [17.030001],\n",
       "       [16.92    ],\n",
       "       [16.879999],\n",
       "       [17.040001],\n",
       "       [17.32    ],\n",
       "       [17.35    ],\n",
       "       [17.92    ],\n",
       "       [18.35    ],\n",
       "       [18.309999],\n",
       "       [18.26    ],\n",
       "       [18.4     ],\n",
       "       [18.42    ],\n",
       "       [19.34    ],\n",
       "       [19.620001],\n",
       "       [19.67    ],\n",
       "       [19.77    ],\n",
       "       [19.74    ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10e175cd-9aef-44fb-b756-87f2849e02dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.497305"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc7bf421-d853-4c78-bcdf-6bf7e469fff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.87454563636364"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_teste.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed428442-aca8-403a-8c4e-7bfb733c77b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3917671954179242"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_teste,previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e246ced-862f-4294-8231-319cd159b33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x733c0d9431a0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_teste,color=\"red\",label=\"Preço real\")\n",
    "plt.plot(previsoes,color=\"blue\",label=\"Previsores\")\n",
    "plt.title(\"Previsão do preço das ações\")\n",
    "plt.xlabel(\"Tempo\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a1719-afe8-4a39-b572-17e7849d7c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
