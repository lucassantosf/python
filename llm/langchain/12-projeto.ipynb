{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Carregar as variáveis do .env\n",
    "\n",
    "# Verificar se a chave foi carregada corretamente\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada no arquivo .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "caminhos = [\n",
    "    \"files/apostila.pdf\",\n",
    "    \"files/LLM.pdf\",\n",
    "    ]\n",
    "\n",
    "paginas = []\n",
    "for caminho in caminhos:\n",
    "    loader = PyPDFLoader(caminho)\n",
    "    paginas.extend(loader.load())\n",
    "\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(paginas)\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos/', '')\n",
    "    doc.metadata['doc_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio = 'arquivos/chat_retrieval_db'\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'O que é Hugging Face e como faço para acessá-lo?',\n",
       " 'result': 'Hugging Face é uma empresa conhecida por seu trabalho em inteligência artificial, especialmente em modelos de linguagem e aprendizado de máquina. Eles oferecem uma biblioteca chamada \"Transformers\", que facilita o uso de modelos de aprendizado profundo, principalmente para tarefas de processamento de linguagem natural. \\n\\nPara acessar o Hugging Face, você pode visitar o site oficial da empresa e explorar a seção de modelos e a documentação da biblioteca. Além disso, você pode instalar a biblioteca Transformers em seu ambiente Python usando o gerenciador de pacotes pip com o comando:\\n\\n```bash\\npip install transformers\\n```\\n\\nDepois de instalada, você pode começar a usar os modelos disponíveis na plataforma.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"O que é Hugging Face e como faço para acessá-lo?\"\n",
    "chat_chain.invoke({\"query\": pergunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chain_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Utilize o contexto fornecido para responder a pergunta ao final. \n",
    "Se você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\n",
    "Utilize três frases no máximo, mantenha a resposta concisa.\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
    "    chain_type_kwargs={\"prompt\":chain_prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face é uma plataforma que fornece modelos de aprendizado de máquina, especialmente para processamento de linguagem natural. Você pode acessá-lo através do seu site oficial ou utilizando a biblioteca \"transformers\" em Python. A documentação disponível facilita o uso e a integração de modelos em seus projetos.\n"
     ]
    }
   ],
   "source": [
    "pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "print(resposta['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Atualmente, requer um pouco mais de esforço para pegar um modelo de código aberto e começar a usá-lo, mas o progresso está ocorrendo muito rapidamente para torná-los mais acessíveis aos usuários. Na Databricks, por exemplo, fizemos melhorias em frameworks de código aberto como o MLflow para tornar muito fácil para alguém com um pouco de experiência em Python pegar qualquer modelo transformador da Hugging Face e usá-lo como um objeto Python. Muitas vezes, você pode encontrar um modelo de código aberto que resolve seu problema específico e que é várias ordens de grandeza menor que o ChatGPT, permitindo que você traga o modelo para seu ambiente e hospede-o você mesmo. Isso significa que você pode manter os dados sob seu controle para preocupações com privacidade e governança, além de gerenciar seus custos. Outra grande vantagem de usar modelos de código aberto é a capacidade de ajustá-los aos seus próprios dados', metadata={'doc_id': 75, 'page': 6, 'source': 'files/LLM.pdf'}),\n",
       " Document(page_content='E-BOOK Um guia compacto sobre Large Language Models (LLM)', metadata={'doc_id': 55, 'page': 0, 'source': 'files/LLM.pdf'}),\n",
       " Document(page_content='. Outras implementações notáveis de IA generativa incluem projetos como a geração de arte a partir de texto, áudio e vídeo, e certamente muitas outras novidades surgirão em breve.', metadata={'doc_id': 58, 'page': 1, 'source': 'files/LLM.pdf'}),\n",
       " Document(page_content='>>> hello()  \\nOlá Mundo!!!  \\n \\n \\n11.2 Parâmetros e a rgumentos  \\n \\n Parâmetros são as variáveis que podem ser incluídas nos parênteses das funções . Quando a \\nfunção é chamada são passados valores para essas variáveis. E sses valores são chamados \\nargumentos. O corpo da função pode utilizar essas variáveis, cujos valores podem modificar o \\ncomportamento da função.  \\n \\nExemplo:  Função para imprimir  o maior entre 2 valores  \\n \\ndef maior(x,y): \\n    if x>y: \\n        print(x) \\n    else: \\n        print(y) \\n \\n>>> maior(4,7)  \\n7', metadata={'doc_id': 46, 'page': 21, 'source': 'files/apostila.pdf'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"O que é Hugging Face e como faço para acessá-lo?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"O que é Hugging Face e como faço para acessá-lo?\",\n",
      "  \"context\": \"Atualmente, requer um pouco mais de esforço para pegar um modelo de código aberto e começar a usá-lo, mas o progresso está ocorrendo muito rapidamente para torná-los mais acessíveis aos usuários. Na Databricks, por exemplo, fizemos melhorias em frameworks de código aberto como o MLflow para tornar muito fácil para alguém com um pouco de experiência em Python pegar qualquer modelo transformador da Hugging Face e usá-lo como um objeto Python. Muitas vezes, você pode encontrar um modelo de código aberto que resolve seu problema específico e que é várias ordens de grandeza menor que o ChatGPT, permitindo que você traga o modelo para seu ambiente e hospede-o você mesmo. Isso significa que você pode manter os dados sob seu controle para preocupações com privacidade e governança, além de gerenciar seus custos. Outra grande vantagem de usar modelos de código aberto é a capacidade de ajustá-los aos seus próprios dados\\n\\nE-BOOK Um guia compacto sobre Large Language Models (LLM)\\n\\n. Outras implementações notáveis de IA generativa incluem projetos como a geração de arte a partir de texto, áudio e vídeo, e certamente muitas outras novidades surgirão em breve.\\n\\n>>> hello()  \\nOlá Mundo!!!  \\n \\n \\n11.2 Parâmetros e a rgumentos  \\n \\n Parâmetros são as variáveis que podem ser incluídas nos parênteses das funções . Quando a \\nfunção é chamada são passados valores para essas variáveis. E sses valores são chamados \\nargumentos. O corpo da função pode utilizar essas variáveis, cujos valores podem modificar o \\ncomportamento da função.  \\n \\nExemplo:  Função para imprimir  o maior entre 2 valores  \\n \\ndef maior(x,y): \\n    if x>y: \\n        print(x) \\n    else: \\n        print(y) \\n \\n>>> maior(4,7)  \\n7\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Utilize o contexto fornecido para responder a pergunta ao final. \\nSe você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\\nUtilize três frases no máximo, mantenha a resposta concisa.\\n\\nContexto: Atualmente, requer um pouco mais de esforço para pegar um modelo de código aberto e começar a usá-lo, mas o progresso está ocorrendo muito rapidamente para torná-los mais acessíveis aos usuários. Na Databricks, por exemplo, fizemos melhorias em frameworks de código aberto como o MLflow para tornar muito fácil para alguém com um pouco de experiência em Python pegar qualquer modelo transformador da Hugging Face e usá-lo como um objeto Python. Muitas vezes, você pode encontrar um modelo de código aberto que resolve seu problema específico e que é várias ordens de grandeza menor que o ChatGPT, permitindo que você traga o modelo para seu ambiente e hospede-o você mesmo. Isso significa que você pode manter os dados sob seu controle para preocupações com privacidade e governança, além de gerenciar seus custos. Outra grande vantagem de usar modelos de código aberto é a capacidade de ajustá-los aos seus próprios dados\\n\\nE-BOOK Um guia compacto sobre Large Language Models (LLM)\\n\\n. Outras implementações notáveis de IA generativa incluem projetos como a geração de arte a partir de texto, áudio e vídeo, e certamente muitas outras novidades surgirão em breve.\\n\\n>>> hello()  \\nOlá Mundo!!!  \\n \\n \\n11.2 Parâmetros e a rgumentos  \\n \\n Parâmetros são as variáveis que podem ser incluídas nos parênteses das funções . Quando a \\nfunção é chamada são passados valores para essas variáveis. E sses valores são chamados \\nargumentos. O corpo da função pode utilizar essas variáveis, cujos valores podem modificar o \\ncomportamento da função.  \\n \\nExemplo:  Função para imprimir  o maior entre 2 valores  \\n \\ndef maior(x,y): \\n    if x>y: \\n        print(x) \\n    else: \\n        print(y) \\n \\n>>> maior(4,7)  \\n7\\n\\nPergunta: O que é Hugging Face e como faço para acessá-lo?\\n\\nResposta:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] [1.87s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hugging Face é uma plataforma que fornece modelos de aprendizado de máquina, especialmente para processamento de linguagem natural. Você pode acessá-lo através do site oficial, onde pode encontrar modelos e bibliotecas, como o Transformers, para integrar em seus projetos. É recomendado ter alguma experiência em Python para utilizar os modelos de forma eficaz.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Hugging Face é uma plataforma que fornece modelos de aprendizado de máquina, especialmente para processamento de linguagem natural. Você pode acessá-lo através do site oficial, onde pode encontrar modelos e bibliotecas, como o Transformers, para integrar em seus projetos. É recomendado ter alguma experiência em Python para utilizar os modelos de forma eficaz.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 66,\n",
      "                \"prompt_tokens\": 461,\n",
      "                \"total_tokens\": 527,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini\",\n",
      "              \"system_fingerprint\": \"fp_72ed7ab54c\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e3b8e14a-207c-4790-a912-97d514487493-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 66,\n",
      "      \"prompt_tokens\": 461,\n",
      "      \"total_tokens\": 527,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"system_fingerprint\": \"fp_72ed7ab54c\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] [1.87s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Hugging Face é uma plataforma que fornece modelos de aprendizado de máquina, especialmente para processamento de linguagem natural. Você pode acessá-lo através do site oficial, onde pode encontrar modelos e bibliotecas, como o Transformers, para integrar em seus projetos. É recomendado ter alguma experiência em Python para utilizar os modelos de forma eficaz.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] [1.87s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Hugging Face é uma plataforma que fornece modelos de aprendizado de máquina, especialmente para processamento de linguagem natural. Você pode acessá-lo através do site oficial, onde pode encontrar modelos e bibliotecas, como o Transformers, para integrar em seus projetos. É recomendado ter alguma experiência em Python para utilizar os modelos de forma eficaz.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA] [2.71s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face é uma empresa e uma comunidade conhecida por seu trabalho em modelos de processamento de linguagem natural (NLP) e aprendizado de máquina, especialmente com modelos pré-treinados como BERT, GPT-2, e muitos outros, incluindo Large Language Models (LLMs). Além de NLP, a Hugging Face também está se expandindo para outras áreas de IA generativa, como a geração de arte a partir de texto, áudio e vídeo.\n",
      "\n",
      "A plataforma oferece uma ampla gama de modelos e ferramentas que facilitam o uso e a implementação de modelos de machine learning, permitindo que desenvolvedores e pesquisadores acessem e utilizem esses recursos de forma simplificada.\n",
      "\n",
      "Para acessar o Hugging Face, você pode seguir estas etapas:\n",
      "\n",
      "1. **Visite o site**: Acesse o site oficial do Hugging Face em [huggingface.co](https://huggingface.co).\n",
      "\n",
      "2. **Explore os modelos**: No site, você pode navegar pela biblioteca de modelos disponíveis, que inclui uma grande variedade de LLMs e outros modelos para tarefas como geração de texto, tradução, classificação, e agora também para a geração de arte e multimídia.\n",
      "\n",
      "3. **Documentação**: Hugging Face oferece uma documentação abrangente que pode ajudá-lo a entender como usar os modelos e as bibliotecas que eles fornecem, como o Transformers. A documentação inclui informações específicas sobre o uso de LLMs, com guias e exemplos práticos. Você pode encontrar a documentação em [huggingface.co/docs](https://huggingface.co/docs).\n",
      "\n",
      "4. **Instalação**: Para começar a usar os modelos em seu código, você pode instalar a biblioteca Transformers usando o gerenciador de pacotes pip. Execute o seguinte comando em seu terminal:\n",
      "   ```\n",
      "   pip install transformers\n",
      "   ```\n",
      "\n",
      "5. **Exemplo de uso**: Após a instalação, você pode usar um modelo de forma simples com algumas linhas de código em Python. Por exemplo, para usar um modelo para gerar texto ou para classificação, você pode definir uma função com parâmetros que aceitam entradas, assim como você faria ao definir funções em Python, como a função `maior` que você mencionou. A documentação contém exemplos práticos que mostram como carregar e usar os LLMs e outros modelos generativos.\n",
      "\n",
      "6. **Comunidade e suporte**: Você também pode se envolver com a comunidade Hugging Face através de fóruns, grupos de discussão e outras plataformas para obter suporte e compartilhar conhecimento.\n",
      "\n",
      "Essas etapas permitirão que você acesse e comece a trabalhar com os recursos oferecidos pelo Hugging Face, especialmente em relação aos Large Language Models e outras inovações em IA generativa.\n"
     ]
    }
   ],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'),\n",
    "    chain_type='refine'\n",
    ")\n",
    "\n",
    "pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "print(resposta['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
