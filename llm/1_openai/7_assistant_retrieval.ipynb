{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai \n",
    "import pandas as pd \n",
    "\n",
    "# Carregar variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar se a chave foi carregada\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY não encontrada no arquivo .env\")\n",
    "\n",
    "# Configurar o cliente OpenAI com a chave carregada\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(\n",
    "    name=\"Tutor de Apostilas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = [\"files/LLM.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stream = [open(f,\"rb\") for f in file]\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    files = file_stream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_batch.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_batch.file_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Tutor Apostila\",\n",
    "    instructions=\"Voce é um tutor especializado em tecnologias emergentes. \\\n",
    "        Voce sabe responder perguntas sobre LLMs como OpenAi, HuggingFace, etc. \\\n",
    "        Caso voce não encontre as respostas, seja sincero e fale que não sabe responder\",\n",
    "    tools=[{\"type\":\"file_search\"}],\n",
    "    tool_resources={\"file_search\":{\"vector_store_ids\":[vector_store.id]}},\n",
    "    model=\"gpt-4-turbo-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"Conforme o documento, o que é HuggingFace?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da Thread\n",
    "thread = client.beta.threads.create()\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=pergunta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa a Thread\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Nome do usuário premium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aguarda a thread rodar\n",
    "import time \n",
    "while run.status in [\"queued\",\"in_progress\",\"cancelling\"]:\n",
    "    time.sleep(1)\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Message](data=[Message(id='msg_mgXb1a4Ht9JtZKf4XY8fqEou', assistant_id='asst_6oSa67orzxifGGrbteKKP4x6', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=1222, file_citation=FileCitation(file_id='file-VAnVp1aS1itthEsKKDYkLa'), start_index=1209, text='【4:0†LLM.pdf】', type='file_citation')], value='HuggingFace é descrita como uma comunidade de código aberto que reúne centenas de milhares de modelos contribuídos por diversos colaboradores. Esses modelos podem ajudar a resolver uma ampla gama de casos de uso específicos, como geração de texto, resumo e classificação. A comunidade de código aberto, incluindo a HuggingFace, está avançando rapidamente para alcançar o desempenho de modelos proprietários, embora ainda não tenha conseguido igualar o desempenho de modelos de ponta como o GPT-4. Os modelos da HuggingFace e outros modelos de código aberto geralmente requerem mais esforço para serem implementados, mas progressos significativos estão sendo feitos para torná-los mais acessíveis aos usuários. Além disso, destacou-se a facilidade com que alguém com um pouco de experiência em Python pode pegar qualquer modelo transformador da HuggingFace e usá-lo como um objeto Python, destacando a flexibilidade e a possibilidade de manter os dados sob controle do usuário para preocupações com privacidade, governança e gestão de custos. A utilização de modelos de código aberto também permite ajustá-los aos dados específicos do usuário, melhorando significativamente o desempenho em domínios específicos【4:0†LLM.pdf】.'), type='text')], created_at=1737751437, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_kg3zjfD2ahfDYViAoUG4vcGB', status=None, thread_id='thread_1NlGj3uqo70CiknuazamTiVG'), Message(id='msg_asqRw1pQmhmzGqN5IXgbzxyo', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Conforme o documento, o que é HuggingFace?'), type='text')], created_at=1737751422, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_1NlGj3uqo70CiknuazamTiVG')], object='list', first_id='msg_mgXb1a4Ht9JtZKf4XY8fqEou', last_id='msg_asqRw1pQmhmzGqN5IXgbzxyo', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "# Verificar a resposta\n",
    "if run.status == \"completed\":\n",
    "    mensagens = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id\n",
    "    )\n",
    "    print(mensagens)\n",
    "else:\n",
    "    print(f\"Erro {run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HuggingFace é descrita como uma comunidade de código aberto que reúne centenas de milhares de modelos contribuídos por diversos colaboradores. Esses modelos podem ajudar a resolver uma ampla gama de casos de uso específicos, como geração de texto, resumo e classificação. A comunidade de código aberto, incluindo a HuggingFace, está avançando rapidamente para alcançar o desempenho de modelos proprietários, embora ainda não tenha conseguido igualar o desempenho de modelos de ponta como o GPT-4. Os modelos da HuggingFace e outros modelos de código aberto geralmente requerem mais esforço para serem implementados, mas progressos significativos estão sendo feitos para torná-los mais acessíveis aos usuários. Além disso, destacou-se a facilidade com que alguém com um pouco de experiência em Python pode pegar qualquer modelo transformador da HuggingFace e usá-lo como um objeto Python, destacando a flexibilidade e a possibilidade de manter os dados sob controle do usuário para preocupações com privacidade, governança e gestão de custos. A utilização de modelos de código aberto também permite ajustá-los aos dados específicos do usuário, melhorando significativamente o desempenho em domínios específicos【4:0†LLM.pdf】.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens.data[0].content[0].text.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando os passos do modelo\n",
    "\n",
    "run_steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====Step tool_calls\n",
      "==========\n",
      "ToolCall não possui 'code_interpreter': FileSearchToolCall(id='call_V760ftqVWZ6Iil5xU8vQDqMY', file_search=FileSearch(ranking_options=FileSearchRankingOptions(ranker='default_2024_08_21', score_threshold=0.0), results=[FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.513213399695081, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.5006500892253628, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.31152131604407635, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.28506882404131934, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.2848309003869383, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.243150085390779, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.24033374748857547, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.2020939250645779, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.1827758230715549, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.17837298833488693, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.14415475712666334, content=None), FileSearchResult(file_id='file-VAnVp1aS1itthEsKKDYkLa', file_name='LLM.pdf', score=0.10764503272848731, content=None)]), type='file_search')\n",
      "==========\n",
      "\n",
      "====Step message_creation\n",
      "==========\n",
      "HuggingFace é descrita como uma comunidade de código aberto que reúne centenas de milhares de modelos contribuídos por diversos colaboradores. Esses modelos podem ajudar a resolver uma ampla gama de casos de uso específicos, como geração de texto, resumo e classificação. A comunidade de código aberto, incluindo a HuggingFace, está avançando rapidamente para alcançar o desempenho de modelos proprietários, embora ainda não tenha conseguido igualar o desempenho de modelos de ponta como o GPT-4. Os modelos da HuggingFace e outros modelos de código aberto geralmente requerem mais esforço para serem implementados, mas progressos significativos estão sendo feitos para torná-los mais acessíveis aos usuários. Além disso, destacou-se a facilidade com que alguém com um pouco de experiência em Python pode pegar qualquer modelo transformador da HuggingFace e usá-lo como um objeto Python, destacando a flexibilidade e a possibilidade de manter os dados sob controle do usuário para preocupações com privacidade, governança e gestão de custos. A utilização de modelos de código aberto também permite ajustá-los aos dados específicos do usuário, melhorando significativamente o desempenho em domínios específicos【4:0†LLM.pdf】.\n"
     ]
    }
   ],
   "source": [
    "for step in run_steps.data[::-1]:\n",
    "    print(F\"\\n====Step {step.step_details.type}\")\n",
    "    if step.step_details.type == \"tool_calls\":\n",
    "        for tool_call in step.step_details.tool_calls:\n",
    "            print(\"=\" * 10)\n",
    "            # Verifique se o tool_call tem o atributo `code_interpreter`\n",
    "            if hasattr(tool_call, \"code_interpreter\"):\n",
    "                print(tool_call.code_interpreter.input)\n",
    "            else:\n",
    "                print(f\"ToolCall não possui 'code_interpreter': {tool_call}\")\n",
    "    print(\"=\" * 10)\n",
    "    if step.step_details.type == \"message_creation\":\n",
    "        message = client.beta.threads.messages.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            message_id=step.step_details.message_creation.message_id\n",
    "        )\n",
    "        print(message.content[0].text.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
