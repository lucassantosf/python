{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"llama-3.3-70b-versatile\",\n",
    "           api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_imposto_renda(rendimento: float) -> str:\n",
    "    \"\"\"\n",
    "    Calcula o imposto de renda com base no rendimento anual.\n",
    "    \n",
    "    Args:\n",
    "        rendimento (float): Rendimento anual do indivíduo.\n",
    "        \n",
    "    Returns:\n",
    "        str: O valor do imposto devido com base no rendimento\n",
    "    \"\"\"\n",
    "    if rendimento <= 2000:\n",
    "        return \"Você está isento de pagar imposto de renda\"\n",
    "    elif 2000 < rendimento <= 5000:\n",
    "        imposto = (rendimento - 2000) * 0.10\n",
    "        return f\"O imposto devido é de R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}\"\n",
    "    elif 5000 < rendimento <= 10000:\n",
    "        imposto = (rendimento - 5000) * 0.15 + 300\n",
    "        return f\"O imposto devido é de R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}\"\n",
    "    else:\n",
    "        imposto = (rendimento - 10000) * 0.20 + 1050\n",
    "        return f\"O imposto devido é de R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo Função em Ferramenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferramenta_imposto_renda = FunctionTool.from_defaults(\n",
    "    fn=calcular_imposto_renda,\n",
    "    name=\"Calcular Imposto de Renda\",\n",
    "    description=(\n",
    "        \"Calcula o imposto de renda com base no rendimento anual.\"\n",
    "        \"Argumento: rendimento (float).\"\n",
    "        \"Retorna o valor do imposto devido de acordo com faixas de rendimento\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker_imposto = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[ferramenta_imposto_renda],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_imposto = AgentRunner(agent_worker_imposto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Qual é o imposto de renda devido por uma pessoa com rendimento\n",
      "    anual de R$ 7.500?\n",
      "    \n",
      "=== Calling Function ===\n",
      "Calling function: Calcular Imposto de Renda with args: {\"rendimento\": 7500}\n",
      "=== Function Output ===\n",
      "O imposto devido é de R$ 675.00, base em um rendimento de R$ 7500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.0 seconds as it raised InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Lamento, mas não tenho como fornecer o valor exato do imposto de renda devido, pois depende de vários fatores, como a faixa de rendimento, deduções e outros. No entanto, posso dizer que o imposto de renda é calculado com base nas faixas de rendimento e alíquotas estabelecidas pela Receita Federal.\n",
      "\n",
      "Se você quiser saber o valor exato do imposto de renda devido, recomendo consultar a tabela de imposto de renda da Receita Federal ou utilizar um simulador de imposto de renda online. Além disso, é importante lembrar que o imposto de renda pode variar de ano para ano, portanto, é importante verificar as informações atualizadas.\n"
     ]
    }
   ],
   "source": [
    "response = agent_imposto.chat(\"\"\"\n",
    "    Qual é o imposto de renda devido por uma pessoa com rendimento\n",
    "    anual de R$ 7.500?\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quem foi Machado de Assis?\n",
      "=== LLM Response ===\n",
      "Machado de Assis foi um escritor, poeta, contista e dramaturgo brasileiro, considerado um dos maiores nomes da literatura brasileira. Ele nasceu em 21 de junho de 1839, no Rio de Janeiro, e faleceu em 29 de setembro de 1908.\n",
      "\n",
      "Machado de Assis é conhecido por suas obras-primas, como \"Dom Casmurro\", \"Memórias Póstumas de Brás Cubas\" e \"Quincas Borba\", que são consideradas algumas das melhores obras da literatura brasileira. Ele foi um dos principais representantes do Realismo e do Naturalismo no Brasil, e sua obra é caracterizada por uma prosa refinada, ironia e uma visão crítica da sociedade brasileira de sua época.\n",
      "\n",
      "Além de sua obra literária, Machado de Assis também foi um jornalista, crítico literário e funcionário público. Ele foi um dos fundadores da Academia Brasileira de Letras e ocupou a cadeira número 23 da instituição.\n",
      "\n",
      "Machado de Assis é considerado um dos mais importantes escritores brasileiros de todos os tempos, e sua obra continua a ser lida e estudada por pessoas de todo o mundo. Ele é lembrado como um mestre da literatura brasileira, e sua influência pode ser vista em muitos outros escritores e artistas brasileiros.\n"
     ]
    }
   ],
   "source": [
    "response = agent_imposto.chat(\"Quem foi Machado de Assis?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalhando com arquivos - Nova Ferramenta Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "\n",
    "def consulta_artigos(titulo: str) -> str:\n",
    "    \"\"\"Consulta os artigos na base de dados ArXiv e retorna resultados formatados.\"\"\"\n",
    "    busca = arxiv.Search(\n",
    "        query=titulo,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    \n",
    "    resultados = [\n",
    "        f\"Título: {artigo.title}\\n\"\n",
    "        f\"Categoria: {artigo.primary_category}\\n\"\n",
    "        f\"Link: {artigo.entry_id}\\n\"\n",
    "        for artigo in busca.results()\n",
    "    ]\n",
    "    \n",
    "    return \"\\n\\n\".join(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta_artigos_tool = FunctionTool.from_defaults(fn=consulta_artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [ferramenta_imposto_renda, consulta_artigos_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LangChain na educação\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain na educa\\u00e7\\u00e3o\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12074/1589355692.py:15: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for artigo in busca.results()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "Título: Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2402.01733v1\n",
      "\n",
      "\n",
      "Título: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "Título: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "Título: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "Título: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "=== LLM Response ===\n",
      "Esses artigos podem ser úteis para entender como o LangChain pode ser aplicado na educação, mas é importante notar que a relação direta com a educação pode não ser explícita em todos os artigos. Além disso, é sempre uma boa prática ler os resumos e as introduções dos artigos para entender melhor o conteúdo e a relevância para o seu interesse específico.\n"
     ]
    }
   ],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "response = agent.chat(\"Me retorne artigos sobre LangChain na educação\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=tavily_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='9d29d3ce-b426-46e1-ac11-ed0edd637269', embedding=None, metadata={'url': 'https://python.langchain.com/v0.1/docs/use_cases/question_answering/sources/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c6bdd72-a10e-416f-b377-88f8b0ad3da5', embedding=None, metadata={'url': 'https://medium.com/@recogna.nlp/descomplicando-agentes-em-langchain-236e856ec687'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='No último artigo, vimos que as chains podem ser usadas para realizar diversas tarefas, já que permitem o encadeamento de vários componentes.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.search(\"Me retorne artigos científicos sobre LangChain\", max_results=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "tavily_tool_function = FunctionTool.from_defaults(\n",
    "    fn=tavily_tool.search,\n",
    "    name=\"Tavily Search\",\n",
    "    description=\"Busca artigos com Tavily sobre um determinado tópico\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[tavily_tool_function],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LLM e LangChain\n",
      "=== Calling Function ===\n",
      "Calling function: Tavily Search with args: {\"query\": \"LLM e LangChain\", \"max_results\": 10}\n",
      "=== Function Output ===\n",
      "[Document(id_='d6f7d86d-0b05-4dde-98c1-9a786efbc1df', embedding=None, metadata={'url': 'https://js.langchain.com/v0.1/docs/modules/chains/foundational/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5c568ddb-6bf1-4c69-b02a-fdcf8dcf5e6f', embedding=None, metadata={'url': 'https://python.langchain.com/v0.1/docs/modules/model_io/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ded38fae-5cfd-4087-9f3e-26b120c4085b', embedding=None, metadata={'url': 'https://stangjarukij.medium.com/llm-essentials-getting-start-with-langchain-and-openai-api-724ca80d6e35'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is a powerful building block for building natural language processing (NLP) applications with large language models (LLMs).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='46b0c4db-ca17-42a0-a784-66cfe71aac7d', embedding=None, metadata={'url': 'https://www.techtarget.com/searchenterpriseai/definition/LangChain'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is an open source framework that enables software developers working with artificial intelligence (AI) and its machine learning subset to combine large language models with other external components to develop LLM-powered applications. LangChain is a framework that simplifies the process of creating generative AI application interfaces. LangChain includes prompt template modules that enable developers to create structured prompts for LLMs. These templates can incorporate examples and specify output formats, facilitating smoother interactions and more accurate responses from the models. The LangChain framework helps developers create LLM-powered applications by offering tools that build intricate workflows and integrate different components. Use LangChain and vector search on Amazon DocumentDB to build a generative AI ...', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='14ecf1bc-3449-4bf3-9af7-5637d625cf1b', embedding=None, metadata={'url': 'https://www.stardog.com/blog/designing-llm-applications-with-knowledge-graphs-and-langchain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain is described as “a framework for developing applications powered by language models” — which is precisely how we use it within', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9af6e7cd-1999-43ce-8e89-ecbf283a2144', embedding=None, metadata={'url': 'https://www.langchain.com/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"LangChain’s suite of products supports developers along each step of the LLM application lifecycle. LangChain products. Debug, collaborate, test, and monitor your LLM app in LangSmith - whether it's built with a LangChain framework or not. LangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises. We couldn’t have achieved \\xa0the product experience delivered to our customers without LangChain, and we couldn’t have done it at the same pace without LangSmith.” LangChain’s suite of products can be used independently or stacked together for multiplicative impact – guiding you through building, running, and managing your LLM apps. Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='33178aff-d441-4fcd-8683-8526d9e05d59', embedding=None, metadata={'url': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"__call__(inputs: Dict[str, Any] | Any, return_only_outputs: bool \\\\= False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None \\\\= None, *, tags: List[str] | None \\\\= None, metadata: Dict[str, Any] | None \\\\= None, run_name: str | None \\\\= None, include_run_info: bool \\\\= False) → Dict[str, Any]# async acall(inputs: Dict[str, Any] | Any, return_only_outputs: bool \\\\= False, callbacks: list[BaseCallbackHandler] | BaseCallbackManager | None \\\\= None, *, tags: List[str] | None \\\\= None, metadata: Dict[str, Any] | None \\\\= None, run_name: str | None \\\\= None, include_run_info: bool \\\\= False) → Dict[str, Any]# configurable_alternatives(which: ConfigurableField, *, default_key: str \\\\= 'default', prefix_keys: bool \\\\= False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) → RunnableSerializable# with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] \\\\= (<class 'Exception'>,), exception_key: Optional[str] \\\\= None) → RunnableWithFallbacksT[Input, Output]#\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ceafa047-e0d7-4b3a-a917-e7d6b01e5a5a', embedding=None, metadata={'url': 'https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Examples using LLMChain¶\\nZapier Natural Language Actions\\nDall-E Image Generator\\nStreamlit Chat Message History\\nArgilla\\nComet\\nAim\\nWeights & Biases\\nSageMaker Tracking\\nRebuff\\nMLflow\\nFlyte\\nChat Over Documents with Vectara\\nVectara Text Generation\\nNatural Language APIs\\nJSON\\nFigma\\nPredibase\\nEden AI\\nAzure ML\\nRemoving logical fallacies from model output\\nAmazon Comprehend Moderation Chain\\nCustom Trajectory Evaluator\\nCustom Pairwise Evaluator\\nSet env var OPENAI_API_KEY or load from a .env file:\\n Set env var OPENAI_API_KEY or load from a .env file\\nRetrieve from vector stores directly\\nImprove document indexing with HyDE\\nStructure answers with OpenAI functions\\nMulti-agent authoritarian speaker selection\\nWebResearchRetriever\\nLost in the middle: The problem with long contexts\\nMemory in LLMChain\\nLogging to file\\nXML Agent\\nDatetime parser\\nPrompt pipelining\\nConnecting to a Feature Store\\nRouter\\nTransformation\\nAsync API Create a new Runnable that retries the original runnable on exceptions.\\nretry_if_exception_type – A tuple of exception types to retry on\\nwait_exponential_jitter – Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt – The maximum number of attempts to make before giving up\\nA new Runnable that retries the original runnable on exceptions.\\n The main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n*args – If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\n The main difference between this method and Chain.__call__ is that this\\nmethod expects inputs to be passed directly in as positional arguments or\\nkeyword arguments, whereas Chain.__call__ expects a single input dictionary\\nwith all the inputs\\n*args – If the chain expects a single input, it can be passed in as the\\nsole positional argument.\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='683b9c26-0339-4dc9-900c-d2a25bc02c80', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Both LLM Chains and LLM Agent Executors offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2aa77d26-110a-48b1-914f-c58fd12a38c8', embedding=None, metadata={'url': 'https://www.tecton.ai/blog/langchain-and-tecton-for-context/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This post explores how to improve AI applications by incorporating up-to-date context derived from feature pipelines using LangChain and Tecton.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "=== LLM Response ===\n",
      "Aqui estão alguns artigos sobre LLM e LangChain:\n",
      "\n",
      "1. \"LLM Essentials: Getting Started with LangChain and OpenAI API\" - Este artigo fornece uma introdução ao LangChain e ao OpenAI API, e como usá-los para construir aplicações de processamento de linguagem natural.\n",
      "2. \"LangChain: A Framework for Developing Applications Powered by Language Models\" - Este artigo descreve o LangChain como um framework para desenvolver aplicações impulsionadas por modelos de linguagem.\n",
      "3. \"Designing LLM Applications with Knowledge Graphs and LangChain\" - Este artigo explora como usar o LangChain e grafos de conhecimento para projetar aplicações de LLM.\n",
      "4. \"LangChain: A Suite of Products for Developing and Managing LLM Applications\" - Este artigo descreve a suíte de produtos do LangChain para desenvolver e gerenciar aplicações de LLM.\n",
      "5. \"Understanding the Differences between LLM Chains and LLM Agent Executors in LangChain\" - Este artigo explica as diferenças entre LLM Chains e LLM Agent Executors no LangChain.\n",
      "6. \"LangChain and Tecton for Context: Improving AI Applications with Up-to-Date Context\" - Este artigo explora como usar o LangChain e o Tecton para melhorar aplicações de IA com contexto atualizado.\n",
      "\n",
      "Esses artigos oferecem uma visão geral do LangChain e de como ele pode ser usado para construir aplicações de LLM, bem como algumas das suas funcionalidades e recursos.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Me retorne artigos sobre LLM e LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui estão alguns artigos sobre LLM e LangChain:\n",
      "\n",
      "1. \"LLM Essentials: Getting Started with LangChain and OpenAI API\" - Este artigo fornece uma introdução ao LangChain e ao OpenAI API, e como usá-los para construir aplicações de processamento de linguagem natural.\n",
      "2. \"LangChain: A Framework for Developing Applications Powered by Language Models\" - Este artigo descreve o LangChain como um framework para desenvolver aplicações impulsionadas por modelos de linguagem.\n",
      "3. \"Designing LLM Applications with Knowledge Graphs and LangChain\" - Este artigo explora como usar o LangChain e grafos de conhecimento para projetar aplicações de LLM.\n",
      "4. \"LangChain: A Suite of Products for Developing and Managing LLM Applications\" - Este artigo descreve a suíte de produtos do LangChain para desenvolver e gerenciar aplicações de LLM.\n",
      "5. \"Understanding the Differences between LLM Chains and LLM Agent Executors in LangChain\" - Este artigo explica as diferenças entre LLM Chains e LLM Agent Executors no LangChain.\n",
      "6. \"LangChain and Tecton for Context: Improving AI Applications with Up-to-Date Context\" - Este artigo explora como usar o LangChain e o Tecton para melhorar aplicações de IA com contexto atualizado.\n",
      "\n",
      "Esses artigos oferecem uma visão geral do LangChain e de como ele pode ser usado para construir aplicações de LLM, bem como algumas das suas funcionalidades e recursos.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Embeddings e Engine de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "url = \"files/LLM.pdf\"\n",
    "artigo = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"files/LLM_2.pdf\"\n",
    "tutorial = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar os Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ad9b1a43864925a27529b1523154ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046c6b83af3348cfaaa36ac7b4a1013d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701472e90b9a4b0abcc3ccaf0ccaabca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c11fa5e8de84f9885487f07028fd560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37796f4385684810a0df10a7ac97560b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21dbb251e7c4cb7883e29a4db7e1ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6294520ef04a4cbbab66908bfc867da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e43302682e40de84a3bd88c42ef9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825cd60a0fc405f8b6558d1f3021ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac83d89ff1b45aab0e77ca24de73789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = \"intfloat/multilingual-e5-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_index = VectorStoreIndex.from_documents(artigo)\n",
    "tutorial_index = VectorStoreIndex.from_documents(tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_index.storage_context.persist(persist_dir=\"artigo\")\n",
    "tutorial_index.storage_context.persist(persist_dir=\"tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"artigo\"\n",
    ")\n",
    "artigo_index = load_index_from_storage(storage_context)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"tutorial\"\n",
    ")\n",
    "tutorial_index = load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_engine = artigo_index.as_query_engine(similarity_top_k=3, llm=llm)\n",
    "tutorial_engine = tutorial_index.as_query_engine(similarity_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=artigo_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"artigo_engine\",\n",
    "            description=(\n",
    "                \"Fornece informações sobre LLM e LangChain.\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=tutorial_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"tutorial_engine\",\n",
    "            description=(\n",
    "                \"Fornece informações sobre casos de uso e aplicações em LLMs.\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais aplicações posso construir com LLM e LangChain?\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicações que você pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Criação e aprimoramento de conteúdo**: Geração automática de texto, assistência na redação, tradução automática, resumo de textos, planejamento e roteiro de conteúdo, brainstorming e programação.\n",
      "2. **Análise e organização de informações**: Análise de sentimento, extração de informações, classificação de textos, revisão técnica e extração de dados específicos de documentos grandes.\n",
      "3. **Interação e automação**: Chatbots, perguntas e respostas, geração de respostas a perguntas com base em um corpus, e automação de tarefas de suporte.\n",
      "4. **Casos de uso em produção**: Chatbots internos, extração de informações, suporte ao centro de atendimento ao cliente, classificação inteligente de documentos, banco conversacional e assistência na elaboração de relatórios de auditoria.\n",
      "\n",
      "Essas aplicações podem ser construídas utilizando as capacidades do LLM e LangChain, que permitem a criação de soluções personalizadas e inovadoras para atender às necessidades específicas de sua empresa ou organização.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicações que você pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Criação e aprimoramento de conteúdo**: como geração automática de texto, assistência na redação, tradução automática, resumo de textos, planejamento e roteiro de conteúdo, brainstorming e programação.\n",
      "2. **Análise e organização de informações**: como análise de sentimento, extração de informações, classificação de textos, revisão técnica e extração de dados específicos de documentos grandes.\n",
      "3. **Interação e automação**: como chatbots, perguntas e respostas, geração de respostas a perguntas com base em um corpus, e automação de tarefas de suporte.\n",
      "4. **Casos de uso em produção**: como chatbots internos, extração de informações, suporte ao centro de atendimento ao cliente, classificação inteligente de documentos, banco conversacional e assistência na elaboração de relatórios de auditoria.\n",
      "\n",
      "Essas aplicações podem ser construídas utilizando a capacidade do LLM de processar e gerar linguagem natural, e a LangChain pode ser usada para integrar e orquestrar essas aplicações em uma solução mais ampla.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicações que você pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Criação e aprimoramento de conteúdo**: Geração automática de texto, assistência na redação, tradução automática, resumo de textos, planejamento e roteiro de conteúdo, brainstorming e programação.\n",
      "2. **Análise e organização de informações**: Análise de sentimento, extração de informações, classificação de textos, revisão técnica e extração de dados específicos de documentos grandes.\n",
      "3. **Interação e automação**: Chatbots, perguntas e respostas, geração de respostas a perguntas com base em um corpus, e automação de tarefas de suporte.\n",
      "4. **Casos de uso em produção**: Chatbots internos, extração de informações, suporte ao centro de atendimento ao cliente, classificação inteligente de documentos, banco conversacional e assistência na elaboração de relatórios de auditoria.\n",
      "\n",
      "Essas aplicações podem ser construídas utilizando a capacidade do LLM de processar e gerar linguagem natural, e a LangChain pode ser usada para integrar e orquestrar essas aplicações em uma solução mais ampla.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicações que você pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Criação e aprimoramento de conteúdo**: Geração automática de texto, assistência na redação, tradução automática, resumo de textos, planejamento e roteiro de conteúdo, brainstorming e programação.\n",
      "2. **Análise e organização de informações**: Análise de sentimento, extração de informações, classificação de textos, revisão técnica e extração de dados específicos de documentos grandes.\n",
      "3. **Interação e automação**: Chatbots, perguntas e respostas, geração de respostas a perguntas com base em um corpus, e automação de tarefas de suporte.\n",
      "4. **Casos de uso em produção**: Chatbots internos, extração de informações, suporte ao centro de atendimento ao cliente, classificação inteligente de documentos, banco conversacional e assistência na elaboração de relatórios de auditoria.\n",
      "\n",
      "Essas aplicações podem ser construídas utilizando a capacidade do LLM de processar e gerar linguagem natural, e a LangChain pode ser usada para integrar e orquestrar essas aplicações em uma solução mais ampla.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLM e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicações que você pode construir com LLM (Large Language Models) e LangChain incluem:\n",
      "\n",
      "1. **Criação e aprimoramento de conteúdo**: como geração automática de texto, assistência na redação, tradução automática, resumo de textos, planejamento e roteiro de conteúdo, e brainstorming.\n",
      "2. **Análise e organização de informações**: como análise de sentimento, extração de informações, classificação de textos, revisão técnica, e extração de dados específicos de documentos grandes.\n",
      "3. **Interação e automação**: como chatbots, perguntas e respostas, e automação de tarefas de suporte.\n",
      "4. **Casos de uso em produção**: como chatbots internos, extração de informações, suporte ao centro de atendimento ao cliente, classificação inteligente de documentos, banco conversacional, e assistência na elaboração de relatórios de auditoria.\n",
      "\n",
      "Essas aplicações podem ser construídas utilizando as capacidades do LLM para processar e gerar linguagem natural, e podem ser integradas com outras tecnologias, como LangChain, para criar soluções mais robustas e personalizadas.\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais aplicações posso construir com LLM e LangChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais tendências em LangChain e LLM?\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendências em LangChain e LLM incluem o uso de modelos de código aberto, que permitem maior controle e personalização, além de serem mais acessíveis. Outra tendência é a capacidade de ajustar esses modelos a dados específicos, melhorando significativamente o desempenho em domínios específicos. Além disso, a importância de ter uma base sólida de dados para o uso eficaz dos LLMs também é uma tendência chave.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendências em LangChain e LLM incluem o uso de modelos de código aberto, que permitem maior controle e personalização, além de serem mais acessíveis. Além disso, a capacidade de ajustar esses modelos a dados específicos é uma grande vantagem, melhorando significativamente o desempenho em domínios específicos. Outra tendência é a importância de ter uma base sólida de dados para implementar e usar os LLMs de forma eficaz.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendências em LangChain e LLM incluem o uso de modelos de código aberto, que permitem maior controle e personalização, além de serem mais acessíveis. Outra tendência é a capacidade de ajustar esses modelos a dados específicos, melhorando significativamente o desempenho em domínios específicos. Além disso, a importância de ter uma base sólida de dados para o uso eficaz dos LLMs também é uma tendência destacada.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendências em LangChain e LLM incluem o uso de modelos de código aberto, que permitem maior controle e personalização, além de serem mais acessíveis. Além disso, a capacidade de ajustar esses modelos a dados específicos é uma grande vantagem, melhorando significativamente o desempenho em domínios específicos. Outra tendência é a importância de ter uma base sólida de dados para implementar e usar os LLMs de forma eficaz.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendências em LangChain e LLM incluem o uso de modelos de código aberto, que permitem maior controle e personalização, além de serem mais acessíveis. Outra tendência é a capacidade de ajustar esses modelos a dados específicos, melhorando significativamente o desempenho em domínios particulares. Além disso, a importância de ter uma base sólida de dados para o uso eficaz dos LLMs é destacada, junto com a necessidade de ferramentas integradas para implementar e ajustar esses modelos.\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais tendências em LangChain e LLM?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 5f906efe-a183-4fa7-b624-6d7e9323ac8e. Step input: Quais as principais ferramentas usadas em LangChain?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: Portuguese. I need to use a tool to help me answer the question.\n",
      "Action: artigo_engine\n",
      "Action Input: {'input': 'Quais as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Não há menção às principais ferramentas usadas em LangChain no contexto fornecido. O contexto discute grandes modelos de linguagem (LLM), sua evolução, aplicações e comparação entre serviços proprietários e modelos de código aberto, mas não menciona LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step 71b1d48c-4215-461d-a16b-81a5452cafc5. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: A ferramenta artigo_engine não forneceu informações suficientes sobre as principais ferramentas usadas em LangChain. Vou tentar novamente com a ferramenta tutorial_engine.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'Quais as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Não há menção específica a \"LangChain\" no contexto fornecido. No entanto, é possível identificar algumas ferramentas e tecnologias relacionadas a LLMs (Modelos de Linguagem Grande) que são mencionadas, como:\n",
      "\n",
      "- Microsoft 365 Copilot\n",
      "- Google Workspace\n",
      "- GitHub Copilot\n",
      "- StarCoder\n",
      "- ELMo (Embeddings from Language Models)\n",
      "- ULMFiT (Universal Language Model Fine-tuning)\n",
      "- GPT, Claude e Gemini\n",
      "\n",
      "Essas ferramentas são usadas para diversas finalidades, incluindo o processamento de linguagem natural, desenvolvimento de software, e integração com outras tecnologias para melhorar a eficiência e a capacidade das empresas.\n",
      "\u001b[0m> Running step 5f6fa54c-e209-4703-a207-a0f8a259e60a. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: A ferramenta tutorial_engine forneceu algumas informações sobre ferramentas relacionadas a LLMs, mas não especificamente sobre LangChain. No entanto, posso inferir que LangChain é uma plataforma que integra LLMs e outras tecnologias para melhorar a eficiência e a capacidade das empresas. Com base nas informações fornecidas, não é possível identificar as principais ferramentas usadas em LangChain, pois não há menção específica a essa plataforma.\n",
      "Answer: Infelizmente, não foi possível identificar as principais ferramentas usadas em LangChain com as ferramentas disponíveis. No entanto, é possível que LangChain utilize algumas das ferramentas e tecnologias relacionadas a LLMs mencionadas, como GPT, ELMo e ULMFiT, para melhorar a eficiência e a capacidade das empresas.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Quais as principais ferramentas usadas em LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 0682b8f6-b0ff-4e2b-bd14-23fea7e03b84. Step input: Quais as principais tendências em LangChain que eu deveria estudar?\n",
      "\u001b[1;3;38;5;200mThought: O usuário está procurando por tendências em LangChain. Para fornecer uma resposta mais precisa, devo usar a ferramenta artigo_engine para obter informações sobre as principais tendências em LangChain.\n",
      "Action: artigo_engine\n",
      "Action Input: {'input': 'principais tendências em LangChain'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Não há informações sobre LangChain nos textos fornecidos. No entanto, é possível identificar algumas tendências relacionadas aos grandes modelos de linguagem (LLMs) que podem estar relacionadas ao contexto de LangChain:\n",
      "\n",
      "1. **Aumento da acessibilidade**: Com o lançamento de interfaces web simples, como o ChatGPT, os LLMs estão se tornando mais acessíveis a um público mais amplo.\n",
      "2. **Melhoria dos dados de treinamento**: A coleta e análise de grandes volumes de dados de alta qualidade estão melhorando drasticamente o desempenho dos modelos.\n",
      "3. **Avanços em técnicas de treinamento**: A integração do feedback humano no processo de treinamento e a utilização de recursos de computação mais poderosos, como GPUs, estão resultando em grandes melhorias de desempenho.\n",
      "4. **Uso de modelos de código aberto**: A capacidade de ajustar modelos de código aberto aos próprios dados e a possibilidade de hospedá-los em ambientes próprios estão se tornando mais comuns.\n",
      "\n",
      "Essas tendências podem estar relacionadas ao contexto de LangChain, mas sem informações específicas sobre LangChain, não é possível fornecer uma resposta mais detalhada.\n",
      "\u001b[0m> Running step 30440b6a-377e-4381-a093-6bfc2e69c36a. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: A ferramenta artigo_engine não forneceu informações específicas sobre LangChain, mas destacou tendências relacionadas a LLMs que podem estar relacionadas ao contexto de LangChain. Agora, devo usar a ferramenta tutorial_engine para obter informações sobre casos de uso e aplicações em LLMs que possam estar relacionados a LangChain.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'casos de uso e aplicações em LLMs relacionados a LangChain'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Não há menção específica a \"LangChain\" nos textos fornecidos. No entanto, os textos descrevem vários casos de uso e aplicações de Large Language Models (LLMs) em diferentes áreas, como:\n",
      "\n",
      "- Chatbots internos para facilitar o acesso a políticas e procedimentos da empresa\n",
      "- Extração de informações de documentos grandes e complexos\n",
      "- Suporte ao centro de atendimento ao cliente\n",
      "- Classificação inteligente de documentos\n",
      "- Banco conversacional para oferecer experiências avançadas de conversação aos clientes\n",
      "- Assistência na elaboração de relatórios de auditoria\n",
      "\n",
      "Além disso, os textos mencionam aplicações mais amplas dos LLMs, como:\n",
      "- Criação e aprimoramento de conteúdo\n",
      "- Análise e organização de informações\n",
      "- Interação e automação\n",
      "\n",
      "Essas aplicações e casos de uso podem estar relacionados a ferramentas ou plataformas que utilizam LLMs, mas não há menção específica a \"LangChain\".\n",
      "\u001b[0m> Running step 9669b9dd-ca6c-471c-9839-986a3673bf19. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: As ferramentas artigo_engine e tutorial_engine não forneceram informações específicas sobre LangChain, mas destacaram tendências, casos de uso e aplicações relacionadas a LLMs que podem estar relacionadas ao contexto de LangChain. Infelizmente, não é possível fornecer uma resposta mais detalhada sobre LangChain com as ferramentas disponíveis.\n",
      "Answer: Infelizmente, não foi possível encontrar informações específicas sobre LangChain com as ferramentas disponíveis. No entanto, as tendências e aplicações relacionadas a LLMs podem estar relacionadas ao contexto de LangChain, como o aumento da acessibilidade, melhoria dos dados de treinamento, avanços em técnicas de treinamento e uso de modelos de código aberto. Além disso, os casos de uso e aplicações em LLMs, como chatbots, extração de informações, suporte ao centro de atendimento ao cliente e criação de conteúdo, podem estar relacionados a ferramentas ou plataformas que utilizam LLMs. É recomendável buscar mais informações sobre LangChain em outras fontes.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Quais as principais tendências em LangChain que eu deveria estudar?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
