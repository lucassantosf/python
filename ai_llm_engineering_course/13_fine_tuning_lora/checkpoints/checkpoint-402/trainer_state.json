{
  "best_global_step": 201,
  "best_metric": 1.0,
  "best_model_checkpoint": "/home/lucas/Projects/python/ai_llm_engineering_course/13_fine_tuning_lora/checkpoints/checkpoint-201",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 402,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04975124378109453,
      "grad_norm": 4.177342891693115,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.6739,
      "step": 10
    },
    {
      "epoch": 0.09950248756218906,
      "grad_norm": 3.9666061401367188,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.6687,
      "step": 20
    },
    {
      "epoch": 0.14925373134328357,
      "grad_norm": 4.1199846267700195,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6656,
      "step": 30
    },
    {
      "epoch": 0.19900497512437812,
      "grad_norm": 4.000033378601074,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.6609,
      "step": 40
    },
    {
      "epoch": 0.24875621890547264,
      "grad_norm": 4.0861639976501465,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6441,
      "step": 50
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 3.0928237438201904,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.6376,
      "step": 60
    },
    {
      "epoch": 0.3482587064676617,
      "grad_norm": 3.9932515621185303,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.6181,
      "step": 70
    },
    {
      "epoch": 0.39800995024875624,
      "grad_norm": 3.769190788269043,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.6032,
      "step": 80
    },
    {
      "epoch": 0.44776119402985076,
      "grad_norm": 3.7327334880828857,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 0.5799,
      "step": 90
    },
    {
      "epoch": 0.4975124378109453,
      "grad_norm": 3.677321434020996,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.562,
      "step": 100
    },
    {
      "epoch": 0.5472636815920398,
      "grad_norm": 3.363359212875366,
      "learning_rate": 4.4e-06,
      "loss": 0.5375,
      "step": 110
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 3.3724513053894043,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.5222,
      "step": 120
    },
    {
      "epoch": 0.6467661691542289,
      "grad_norm": 3.2279233932495117,
      "learning_rate": 5.2e-06,
      "loss": 0.4857,
      "step": 130
    },
    {
      "epoch": 0.6965174129353234,
      "grad_norm": 3.1249256134033203,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.458,
      "step": 140
    },
    {
      "epoch": 0.746268656716418,
      "grad_norm": 2.977032423019409,
      "learning_rate": 6e-06,
      "loss": 0.4204,
      "step": 150
    },
    {
      "epoch": 0.7960199004975125,
      "grad_norm": 2.8021247386932373,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.3921,
      "step": 160
    },
    {
      "epoch": 0.845771144278607,
      "grad_norm": 2.698758125305176,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.3545,
      "step": 170
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 2.3561501502990723,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.3173,
      "step": 180
    },
    {
      "epoch": 0.945273631840796,
      "grad_norm": 2.2629830837249756,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.2743,
      "step": 190
    },
    {
      "epoch": 0.9950248756218906,
      "grad_norm": 2.0689644813537598,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.2364,
      "step": 200
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 1.0,
      "eval_loss": 0.19261625409126282,
      "eval_runtime": 46.5664,
      "eval_samples_per_second": 8.633,
      "eval_steps_per_second": 1.095,
      "step": 201
    },
    {
      "epoch": 1.044776119402985,
      "grad_norm": 1.8789678812026978,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.2117,
      "step": 210
    },
    {
      "epoch": 1.0945273631840795,
      "grad_norm": 1.6447763442993164,
      "learning_rate": 8.8e-06,
      "loss": 0.1556,
      "step": 220
    },
    {
      "epoch": 1.144278606965174,
      "grad_norm": 1.2220046520233154,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.1137,
      "step": 230
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.8926828503608704,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0788,
      "step": 240
    },
    {
      "epoch": 1.243781094527363,
      "grad_norm": 1.4179641008377075,
      "learning_rate": 1e-05,
      "loss": 0.0822,
      "step": 250
    },
    {
      "epoch": 1.2935323383084576,
      "grad_norm": 0.383019357919693,
      "learning_rate": 1.04e-05,
      "loss": 0.0314,
      "step": 260
    },
    {
      "epoch": 1.3432835820895521,
      "grad_norm": 0.3318488299846649,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 0.0216,
      "step": 270
    },
    {
      "epoch": 1.3930348258706466,
      "grad_norm": 0.23394285142421722,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0144,
      "step": 280
    },
    {
      "epoch": 1.4427860696517412,
      "grad_norm": 0.15624797344207764,
      "learning_rate": 1.16e-05,
      "loss": 0.0102,
      "step": 290
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 0.1468033343553543,
      "learning_rate": 1.2e-05,
      "loss": 0.0967,
      "step": 300
    },
    {
      "epoch": 1.5422885572139302,
      "grad_norm": 0.1462584286928177,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 0.0075,
      "step": 310
    },
    {
      "epoch": 1.5920398009950247,
      "grad_norm": 0.09971760958433151,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.047,
      "step": 320
    },
    {
      "epoch": 1.6417910447761193,
      "grad_norm": 0.12679903209209442,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 0.0051,
      "step": 330
    },
    {
      "epoch": 1.6915422885572138,
      "grad_norm": 0.09013962000608444,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0044,
      "step": 340
    },
    {
      "epoch": 1.7412935323383083,
      "grad_norm": 0.06678209453821182,
      "learning_rate": 1.4e-05,
      "loss": 0.0037,
      "step": 350
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.04702747240662575,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.0031,
      "step": 360
    },
    {
      "epoch": 1.8407960199004973,
      "grad_norm": 0.047408539801836014,
      "learning_rate": 1.48e-05,
      "loss": 0.0025,
      "step": 370
    },
    {
      "epoch": 1.890547263681592,
      "grad_norm": 0.043900758028030396,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.0024,
      "step": 380
    },
    {
      "epoch": 1.9402985074626866,
      "grad_norm": 0.03450814262032509,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 0.0018,
      "step": 390
    },
    {
      "epoch": 1.9900497512437811,
      "grad_norm": 0.03671230003237724,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0016,
      "step": 400
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 1.0,
      "eval_loss": 0.0012487880885601044,
      "eval_runtime": 32.6981,
      "eval_samples_per_second": 12.294,
      "eval_steps_per_second": 1.56,
      "step": 402
    }
  ],
  "logging_steps": 10,
  "max_steps": 603,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 109286984159232.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
